{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wals_tft.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "M44gBnBk0b4H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Collaborative filtering on Google Analytics data (operationalized)\n",
        "\n",
        "This notebook demonstrates how to implement a WALS matrix refactorization approach to do collaborative filtering. Unlike [wals.ipynb](wals.ipynb), this notebook uses TensorFlow Transform to carry out the preprocessing. This way, these steps are automated:\n",
        "* Mapping visitorId (a string) to userId (an enumeration)\n",
        "* Mapping contentId (a string) to itemId (an enumeration)\n",
        "* Removing already viewed items from the batch prediction output\n",
        "* Replacing the userId and visitorId in batch prediction output by visitorId and contentId"
      ]
    },
    {
      "metadata": {
        "id": "1d3q7VSb0b4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apache Beam only works in Python 2 at the moment, so we're going to switch to the Python 2 kernel. In the above menu, click the dropdown arrow and select `python2`. ![image.png](attachment:image.png)"
      ]
    },
    {
      "metadata": {
        "id": "q9e86PQ-0b4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Only [specific combinations](https://github.com/tensorflow/transform#compatible-versions) of TensorFlow, Beam and TensorFlow Transform are supported.  After running the following cell, Reset the notebook."
      ]
    },
    {
      "metadata": {
        "id": "F1-qoMhS0b4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2230
        },
        "outputId": "e2249e59-bab7-4758-e1db-2a0753489133"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "source activate py2env\n",
        "pip uninstall -y google-cloud-dataflow\n",
        "pip install pip==9.0.3\n",
        "conda install -y pytz==2018.4\n",
        "pip install datalab\n",
        "pip install apache-beam[gcp] tensorflow_transform==0.8.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip==9.0.3 in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: datalab in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: urllib3>=1.22 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: pyyaml>=3.11 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: oauth2client>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: plotly>=1.12.5 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: futures>=3.0.5; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: google-cloud>=0.30.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: pandas-profiling>=1.0.0a2 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: six==1.10.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: ipykernel>=4.5.2 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: scikit-learn>=0.18.2 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: scikit-image>=0.13.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: psutil>=4.3.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: seaborn>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: httplib2>=0.10.3 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: pytz>=2015.4 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: configparser>=3.5.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.2 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.2 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python2.7/dist-packages (from datalab)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->datalab)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->datalab)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=2.2.0->datalab)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=2.2.0->datalab)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=2.2.0->datalab)\n",
            "Requirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python2.7/dist-packages (from plotly>=1.12.5->datalab)\n",
            "Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python2.7/dist-packages (from plotly>=1.12.5->datalab)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python2.7/dist-packages (from plotly>=1.12.5->datalab)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.9.1->datalab)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.9.1->datalab)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.9.1->datalab)\n",
            "Requirement already satisfied: jinja2>=2.8 in /usr/local/lib/python2.7/dist-packages (from pandas-profiling>=1.0.0a2->datalab)\n",
            "Requirement already satisfied: matplotlib>=1.4 in /usr/local/lib/python2.7/dist-packages (from pandas-profiling>=1.0.0a2->datalab)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python2.7/dist-packages (from scikit-learn>=0.18.2->datalab)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scikit-learn>=0.18.2->datalab)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python2.7/dist-packages (from scikit-image>=0.13.0->datalab)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python2.7/dist-packages (from scikit-image>=0.13.0->datalab)\n",
            "Requirement already satisfied: pillow>=2.1.0 in /usr/local/lib/python2.7/dist-packages (from scikit-image>=0.13.0->datalab)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python2.7/dist-packages (from google-auth-httplib2>=0.0.2->datalab)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client>=1.6.2->datalab)\n",
            "Requirement already satisfied: functools32; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from jsonschema>=2.6.0->datalab)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python2.7/dist-packages (from nbformat>=4.2->plotly>=1.12.5->datalab)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python2.7/dist-packages (from nbformat>=4.2->plotly>=1.12.5->datalab)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python2.7/dist-packages (from jinja2>=2.8->pandas-profiling>=1.0.0a2->datalab)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4->pandas-profiling>=1.0.0a2->datalab)\n",
            "Requirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4->pandas-profiling>=1.0.0a2->datalab)\n",
            "Requirement already satisfied: subprocess32 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4->pandas-profiling>=1.0.0a2->datalab)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4->pandas-profiling>=1.0.0a2->datalab)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from matplotlib>=1.4->pandas-profiling>=1.0.0a2->datalab)\n",
            "Requirement already satisfied: enum34; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.1.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: backports.shutil-get-terminal-size; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: pathlib2; python_version == \"2.7\" or python_version == \"3.3\" in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python2.7/dist-packages (from ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python2.7/dist-packages (from jupyter-client->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: singledispatch in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: backports_abc>=0.4 in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from pillow>=2.1.0->scikit-image>=0.13.0->datalab)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth->google-auth-httplib2>=0.0.2->datalab)\n",
            "Requirement already satisfied: scandir; python_version < \"3.5\" in /usr/local/lib/python2.7/dist-packages (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python2.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python2.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel>=4.5.2->datalab)\n",
            "Requirement already satisfied: apache-beam[gcp] in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: tensorflow_transform==0.8.0 in /usr/local/lib/python2.7/dist-packages\n",
            "Requirement already satisfied: pyyaml<4.0.0,>=3.12 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: futures<4.0.0,>=3.1.1 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: httplib2<=0.11.3,>=0.8 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: dill<=0.2.8.2,>=0.2.6 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: future<1.0.0,>=0.16.0 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: fastavro<0.22,>=0.21.4 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: typing<3.7.0,>=3.6.0; python_version < \"3.5.0\" in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: pytz<=2018.4,>=2018.3 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: avro<2.0.0,>=1.8.1 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: protobuf<4,>=3.5.0.post1 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: pydot<1.3,>=1.2.0 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: pyvcf<0.7.0,>=0.6.8 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: google-cloud-pubsub==0.35.4; extra == \"gcp\" in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\" in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\" in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: googledatastore<7.1,>=7.0.1; python_version < \"3.0\" and extra == \"gcp\" in /usr/local/lib/python2.7/dist-packages (from apache-beam[gcp])\n",
            "Requirement already satisfied: six<2,>=1.10 in /usr/local/lib/python2.7/dist-packages (from tensorflow_transform==0.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow_transform==0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow_transform==0.8.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python2.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python2.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp])\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp])\n",
            "Requirement already satisfied: enum34>=1.0.4 in /usr/local/lib/python2.7/dist-packages (from grpcio<2,>=1.8->apache-beam[gcp])\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf<4,>=3.5.0.post1->apache-beam[gcp])\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python2.7/dist-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp])\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp])\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp])\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp])\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp])\n",
            "Requirement already satisfied: grpc-google-iam-v1<0.12dev,>=0.11.1 in /usr/local/lib/python2.7/dist-packages (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp])\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.5.2 in /usr/local/lib/python2.7/dist-packages (from proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\"->apache-beam[gcp])\n",
            "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python2.7/dist-packages (from google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp])\n",
            "Requirement already satisfied: google-cloud-core<0.30dev,>=0.28.0 in /usr/local/lib/python2.7/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp])\n",
            "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp])\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python2.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp])\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python2.7/dist-packages (from fasteners>=0.14->google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp])\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bash: line 1: activate: No such file or directory\n",
            "Cannot uninstall requirement google-cloud-dataflow, not installed\n",
            "bash: line 4: conda: command not found\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uryyXYLc0b4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1e5e55c8-f1e1-4379-f0ea-47d071dc942f"
      },
      "cell_type": "code",
      "source": [
        "!pip freeze | grep -e 'flow\\|beam'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apache-beam==2.9.0\n",
            "mesh-tensorflow==0.0.5\n",
            "tensorflow==1.13.1\n",
            "tensorflow-estimator==1.13.0\n",
            "tensorflow-hub==0.2.0\n",
            "tensorflow-metadata==0.12.1\n",
            "tensorflow-probability==0.6.0\n",
            "tensorflow-transform==0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VIAYTDuh0b4Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "PROJECT = 'qwiklabs-gcp-4a684069c4776675'\n",
        "BUCKET = 'colaborative-filtering-agea'\n",
        "REGION = 'us-central1'\n",
        "\n",
        "# do not change these\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['REGION'] = REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ufJRJoUU0b4S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2e91dc8e-53cd-4be3-bc9f-d66ae53eaeab"
      },
      "cell_type": "code",
      "source": [
        "!gcloud config set project $PROJECT\n",
        "!gcloud config set compute/region $REGION"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gT0tFiXK0b4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0a38d79-bbc2-4618-bdbe-bf80d73e0b6f"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print tf.__version__\n",
        "! echo \"tensorflow-transform==0.8.0\" > requirements.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O0c9V4sH5sF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80dff657-2430-4f41-bd44-ced24865511e"
      },
      "cell_type": "code",
      "source": [
        "! cat requirements.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow-transform==0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kbbJ0thu0b4V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create dataset for WALS using TF Transform\n",
        "<p>\n",
        "For collaborative filtering, we don't need to know anything about either the users or the content. Essentially, all we need to know is userId, itemId, and rating that the particular user gave the particular item.\n",
        "<p>\n",
        "In this case, we are working with newspaper articles. The company doesn't ask their users to rate the articles. However, we can use the time-spent on the page as a proxy for rating.\n",
        "<p>\n",
        "Normally, we would also add a time filter to this (\"latest 7 days\"), but our dataset is itself limited to a few days."
      ]
    },
    {
      "metadata": {
        "id": "ECX2nkzQ0b4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "#standardSQL\n",
        "SELECT \n",
        "  Dataset_B.user_id as visitorId,\n",
        "  Dataset_A.content_id as contentId,\n",
        "  COUNT(Dataset_B.user_id) as numberOfVisits\n",
        "\n",
        "  FROM AGEA_ASL.Dataset_A LEFT JOIN AGEA_ASL.Dataset_B \n",
        "  ON Dataset_A.content_id = Dataset_B.content_id\n",
        "  GROUP BY contentId,visitorId\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AAsmo7y40b4Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import google.datalab.bigquery as bq\n",
        "#df = bq.Query(query + \" LIMIT 100\").execute().result().to_dataframe()\n",
        "#df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJxYwsmU1KZk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "24c47853-f59a-47ba-bca2-32223c7b44ae"
      },
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "client = bigquery.Client(PROJECT)\n",
        "df = client.query(query + 'LIMIT 1000').to_dataframe() # TODO: SACAR LIMIT\n",
        "df.head()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>visitorId</th>\n",
              "      <th>contentId</th>\n",
              "      <th>numberOfVisits</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5532689</td>\n",
              "      <td>1CJUzbJaF</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1676769</td>\n",
              "      <td>pPjvz7lWT</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3771589</td>\n",
              "      <td>-m3ccD3Tn</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6220156</td>\n",
              "      <td>TFl5dXr-O</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6907334</td>\n",
              "      <td>xalDSPP-V</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  visitorId  contentId  numberOfVisits\n",
              "0   5532689  1CJUzbJaF               1\n",
              "1   1676769  pPjvz7lWT               1\n",
              "2   3771589  -m3ccD3Tn               1\n",
              "3   6220156  TFl5dXr-O               1\n",
              "4   6907334  xalDSPP-V               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "WuHLnwNg0b4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3006
        },
        "outputId": "e2d9fedb-3b19-44ac-87df-e0572dfb9888"
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import tensorflow as tf\n",
        "import apache_beam as beam\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.beam import impl as beam_impl\n",
        "\n",
        "def preprocess_tft(rowdict):\n",
        "    median = 1 #tft.quantiles(rowdict['session_duration'], 11, epsilon=0.001)[5]\n",
        "    result = {\n",
        "      'userId' : tft.string_to_int(rowdict['visitorId'], vocab_filename='vocab_users'),\n",
        "      'itemId' : tft.string_to_int(rowdict['contentId'], vocab_filename='vocab_items'),\n",
        "      'rating' : 0.3 * rowdict['numberOfVisits'] / median\n",
        "    }\n",
        "    # cap the rating at 1.0\n",
        "    result['rating'] = tf.where(tf.less(result['rating'], tf.ones(tf.shape(result['rating']))),\n",
        "                               result['rating'], tf.ones(tf.shape(result['rating'])))\n",
        "    return result\n",
        "  \n",
        "def preprocess(query, in_test_mode):\n",
        "  import os\n",
        "  import os.path\n",
        "  import tempfile\n",
        "  import tensorflow as tf\n",
        "  from apache_beam.io import tfrecordio\n",
        "  from tensorflow_transform.coders import example_proto_coder\n",
        "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "  from tensorflow_transform.tf_metadata import dataset_schema\n",
        "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
        "\n",
        "  def write_count(a, outdir, basename):\n",
        "    filename = os.path.join(outdir, basename)\n",
        "    (a \n",
        "         | '{}_1'.format(basename) >> beam.Map(lambda x: (1, 1)) \n",
        "         | '{}_2'.format(basename) >> beam.combiners.Count.PerKey()\n",
        "         | '{}_3'.format(basename) >> beam.Map(lambda (k, v): v)\n",
        "         | '{}_write'.format(basename) >> beam.io.WriteToText(file_path_prefix=filename, num_shards=1))\n",
        "\n",
        "  def to_tfrecord(key_vlist, indexCol):\n",
        "    (key, vlist) = key_vlist\n",
        "    return {\n",
        "      'key': [key],\n",
        "      'indices': [value[indexCol] for value in vlist],\n",
        "      'values':  [value['rating'] for value in vlist]\n",
        "    }\n",
        "  \n",
        "  job_name = 'preprocess-wals-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
        "  if in_test_mode:\n",
        "    import shutil\n",
        "    print 'Launching local job ... hang on'\n",
        "    OUTPUT_DIR = './preproc_tft'\n",
        "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
        "  else:\n",
        "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
        "    OUTPUT_DIR = 'gs://{0}/wals/preproc_tft/'.format(BUCKET)\n",
        "    import subprocess\n",
        "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
        "    \n",
        "  options = {\n",
        "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
        "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
        "    'job_name': job_name,\n",
        "    'project': PROJECT,\n",
        "    'max_num_workers': 24,\n",
        "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
        "    'save_main_session': False,\n",
        "    'requirements_file': 'requirements.txt'\n",
        "  }\n",
        "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
        "  if in_test_mode:\n",
        "    RUNNER = 'DirectRunner'\n",
        "  else:\n",
        "    RUNNER = 'DataflowRunner'\n",
        "\n",
        "  # set up metadata  \n",
        "  raw_data_schema = {\n",
        "    colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
        "                   for colname in 'visitorId,contentId'.split(',')\n",
        "  }\n",
        "  raw_data_schema.update({\n",
        "      colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
        "                   for colname in 'numberOfVisits'.split(',')\n",
        "    })\n",
        "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
        " \n",
        "  # run Beam  \n",
        "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
        "    with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
        "      # read raw data\n",
        "      selquery = query\n",
        "      if in_test_mode:\n",
        "         selquery = selquery + ' LIMIT 100'\n",
        "      raw_data = (p \n",
        "                  | 'read' >> beam.io.Read(beam.io.BigQuerySource(query=selquery, use_standard_sql=True)))\n",
        "  \n",
        "      # analyze and transform\n",
        "      raw_dataset = (raw_data, raw_data_metadata)\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))     \n",
        "      transformed_data, transformed_metadata = transformed_dataset\n",
        "      _ = (transform_fn\n",
        "           | 'WriteTransformFn' >>\n",
        "           transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'transform_fn')))\n",
        "      \n",
        "      # do a group-by to create users_for_item and items_for_user\n",
        "      users_for_item = (transformed_data\n",
        "                        | 'map_items' >> beam.Map(lambda x : (x['itemId'], x))\n",
        "                        | 'group_items' >> beam.GroupByKey()\n",
        "                        | 'totfr_items' >> beam.Map(lambda item_userlist : to_tfrecord(item_userlist, 'userId')))\n",
        "      items_for_user = (transformed_data\n",
        "                        | 'map_users' >> beam.Map(lambda x : (x['userId'], x))\n",
        "                        | 'group_users' >> beam.GroupByKey()\n",
        "                        | 'totfr_users' >> beam.Map(lambda item_userlist : to_tfrecord(item_userlist, 'itemId')))\n",
        "      \n",
        "      output_schema = {\n",
        "        'key' : dataset_schema.ColumnSchema(tf.int64, [1], dataset_schema.FixedColumnRepresentation()),\n",
        "        'indices': dataset_schema.ColumnSchema(tf.int64, [], dataset_schema.ListColumnRepresentation()),\n",
        "        'values': dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.ListColumnRepresentation())\n",
        "      }\n",
        "\n",
        "      _ = users_for_item | 'users_for_item' >> tfrecordio.WriteToTFRecord(\n",
        "          os.path.join(OUTPUT_DIR, 'users_for_item'),\n",
        "          coder=example_proto_coder.ExampleProtoCoder(\n",
        "              dataset_schema.Schema(output_schema)))\n",
        "      _ = items_for_user | 'items_for_user' >> tfrecordio.WriteToTFRecord(\n",
        "          os.path.join(OUTPUT_DIR, 'items_for_user'),\n",
        "          coder=example_proto_coder.ExampleProtoCoder(\n",
        "              dataset_schema.Schema(output_schema)))\n",
        "      \n",
        "      write_count(users_for_item, OUTPUT_DIR, 'nitems')\n",
        "      write_count(items_for_user, OUTPUT_DIR, 'nusers') \n",
        "     \n",
        "preprocess(query, in_test_mode=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Launching Dataflow job preprocess-wals-features-190304-234630 ... hang on\n",
            "WARNING:tensorflow:From <ipython-input-9-c3752b485427>:10: string_to_int (from tensorflow_transform.mappers) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tft.compute_and_apply_vocabulary()` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: gs://colaborative-filtering-agea/wals/preproc_tft/tmp/tftransform_tmp/860c984413d64d6181ccaa26ffafba7d/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py:800: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
            "  options = pbegin.pipeline.options.view_as(DebugOptions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: gs://colaborative-filtering-agea/wals/preproc_tft/tmp/tftransform_tmp/cf9aaab42ffd404e94dd96e24af33637/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Retry with exponential backoff: waiting for 4.45102960766 seconds before retrying _populate_requirements_cache because we caught exception: CalledProcessError: Command '['/usr/bin/python2', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', 'requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']' returned non-zero exit status 1\n",
            " Traceback for above exception (most recent call last):\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py\", line 184, in wrapper\n",
            "    return fun(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/runners/portability/stager.py\", line 426, in _populate_requirements_cache\n",
            "    processes.check_output(cmd_args, stderr=processes.STDOUT)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/utils/processes.py\", line 53, in check_output\n",
            "    return subprocess.check_output(*args, **kwargs)\n",
            "  File \"/usr/lib/python2.7/subprocess.py\", line 223, in check_output\n",
            "    raise CalledProcessError(retcode, cmd, output=output)\n",
            "\n",
            "WARNING:root:Retry with exponential backoff: waiting for 7.69550407679 seconds before retrying _populate_requirements_cache because we caught exception: CalledProcessError: Command '['/usr/bin/python2', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', 'requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']' returned non-zero exit status 1\n",
            " Traceback for above exception (most recent call last):\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py\", line 184, in wrapper\n",
            "    return fun(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/runners/portability/stager.py\", line 426, in _populate_requirements_cache\n",
            "    processes.check_output(cmd_args, stderr=processes.STDOUT)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/utils/processes.py\", line 53, in check_output\n",
            "    return subprocess.check_output(*args, **kwargs)\n",
            "  File \"/usr/lib/python2.7/subprocess.py\", line 223, in check_output\n",
            "    raise CalledProcessError(retcode, cmd, output=output)\n",
            "\n",
            "WARNING:root:Retry with exponential backoff: waiting for 13.6434225943 seconds before retrying _populate_requirements_cache because we caught exception: CalledProcessError: Command '['/usr/bin/python2', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', 'requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']' returned non-zero exit status 1\n",
            " Traceback for above exception (most recent call last):\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py\", line 184, in wrapper\n",
            "    return fun(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/runners/portability/stager.py\", line 426, in _populate_requirements_cache\n",
            "    processes.check_output(cmd_args, stderr=processes.STDOUT)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/utils/processes.py\", line 53, in check_output\n",
            "    return subprocess.check_output(*args, **kwargs)\n",
            "  File \"/usr/lib/python2.7/subprocess.py\", line 223, in check_output\n",
            "    raise CalledProcessError(retcode, cmd, output=output)\n",
            "\n",
            "WARNING:root:Retry with exponential backoff: waiting for 27.9816397564 seconds before retrying _populate_requirements_cache because we caught exception: CalledProcessError: Command '['/usr/bin/python2', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', 'requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']' returned non-zero exit status 1\n",
            " Traceback for above exception (most recent call last):\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py\", line 184, in wrapper\n",
            "    return fun(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/runners/portability/stager.py\", line 426, in _populate_requirements_cache\n",
            "    processes.check_output(cmd_args, stderr=processes.STDOUT)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/apache_beam/utils/processes.py\", line 53, in check_output\n",
            "    return subprocess.check_output(*args, **kwargs)\n",
            "  File \"/usr/lib/python2.7/subprocess.py\", line 223, in check_output\n",
            "    raise CalledProcessError(retcode, cmd, output=output)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c3752b485427>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m       \u001b[0mwrite_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_for_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nusers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_test_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-c3752b485427>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(query, in_test_mode)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0mwrite_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers_for_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nitems'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m       \u001b[0mwrite_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_for_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nusers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_test_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m           self._options).run(False)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_type_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.pyc\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# raise an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     result = DataflowPipelineResult(\n\u001b[0;32m--> 394\u001b[0;31m         self.dataflow_client.create_job(self.job), self)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;31m# TODO(BEAM-4274): Circular import runners-metrics. Requires refactoring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36mcreate_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    498\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;34m\"\"\"Creates job description. May stage and/or submit for remote execution.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;31m# Stage and submit the job when necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36mcreate_job_description\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;31m# Stage other resources for the SDK harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m     \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stage_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     job.proto.environment = Environment(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36m_stage_resources\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mtemp_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         staging_location=google_cloud_options.staging_location)\n\u001b[0m\u001b[1;32m    463\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/runners/portability/stager.pyc\u001b[0m in \u001b[0;36mstage_job_resources\u001b[0;34m(self, options, build_setup_args, temp_dir, populate_requirements_cache, staging_location)\u001b[0m\n\u001b[1;32m    164\u001b[0m       (populate_requirements_cache if populate_requirements_cache else\n\u001b[1;32m    165\u001b[0m        \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_populate_requirements_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetup_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequirements_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                             requirements_cache_path)\n\u001b[0m\u001b[1;32m    167\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements_cache_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         self.stage_artifact(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m               \u001b[0;31m# Re-raise the original exception since we finished the retries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m               \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexn_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             logger(\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/runners/portability/stager.pyc\u001b[0m in \u001b[0;36m_populate_requirements_cache\u001b[0;34m(requirements_file, cache_dir)\u001b[0m\n\u001b[1;32m    424\u001b[0m     ]\n\u001b[1;32m    425\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Executing command: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/apache_beam/utils/processes.pyc\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mforce_shell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/usr/bin/python2', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', 'requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']' returned non-zero exit status 1"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "2ourkgyt0b4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39723e6c-99d5-4b45-8885-f9da9a5f0345"
      },
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://${BUCKET}/wals/preproc_tft/\n",
        "  \n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BadRequestException: 400 Invalid bucket name: '-filtering-agea'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H_Li5f650b4e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%bash\n",
        "gsutil ls gs://${BUCKET}/wals/preproc_tft/transform_fn/transform_fn/\n",
        "gsutil ls gs://${BUCKET}/wals/preproc_tft/transform_fn/transform_fn/assets/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YC5NAlkv0b4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To summarize, we created the following data files from the BiqQuery resultset:\n",
        "<ol>\n",
        "<li> ```users_for_item``` contains all the users/ratings for each item in TFExample format.  The items and users here are integers (not strings) i.e. itemId not contentId and userId not visitorId. The rating is scaled.\n",
        "<li> ```items_for_user``` contains all the items/ratings for each user in TFExample format.  The items and users here are integers (not strings) i.e. itemId not contentId and userId not visitorId. The rating is scaled.\n",
        "<li> ```vocab_items``` contains the mapping from the contentId to the enumerated itemId\n",
        "<li> ```vocab_items``` contains the mapping from the visitorId to the enumerated userId\n",
        "</ol>"
      ]
    },
    {
      "metadata": {
        "id": "3Ixnq3uC0b4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train with WALS\n",
        "\n",
        "Once you have the dataset, do matrix factorization with WALS using the [WALSMatrixFactorization](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/factorization/WALSMatrixFactorization) in the contrib directory.\n",
        "This is an estimator model, so it should be relatively familiar.\n",
        "<p>\n",
        "As usual, we write an input_fn to provide the data to the model, and then create the Estimator to do train_and_evaluate.\n",
        "Because it is in contrib and hasn't moved over to tf.estimator yet, we use tf.contrib.learn.Experiment to handle the training loop."
      ]
    },
    {
      "metadata": {
        "id": "1BamuAMQ0b4h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from tensorflow.contrib.factorization import WALSMatrixFactorization\n",
        "\n",
        "def read_dataset(mode, args):\n",
        "  def decode_example(protos, vocab_size):\n",
        "    features = {'key': tf.FixedLenFeature([1], tf.int64),\n",
        "                'indices': tf.VarLenFeature(dtype=tf.int64),\n",
        "                'values': tf.VarLenFeature(dtype=tf.float32)}\n",
        "    parsed_features = tf.parse_single_example(protos, features)\n",
        "    values = tf.sparse_merge(parsed_features['indices'], parsed_features['values'], vocab_size=vocab_size)\n",
        "    # Save key to remap after batching\n",
        "    key = parsed_features['key']\n",
        "    decoded_sparse_tensor = tf.SparseTensor(indices=tf.concat([values.indices, [key]], axis = 0), values = tf.concat([values.values, [0.0]], axis = 0), dense_shape = values.dense_shape)\n",
        "    return decoded_sparse_tensor\n",
        "  \n",
        "  \n",
        "  def remap_keys(sparse_tensor):\n",
        "    # Current indices of our SparseTensor that we need to fix\n",
        "    bad_indices = sparse_tensor.indices\n",
        "    # Current values of our SparseTensor that we need to fix\n",
        "    bad_values = sparse_tensor.values \n",
        "  \n",
        "    # Group by the batch_indices and get the count for each  \n",
        "    size = tf.segment_sum(data = tf.ones_like(bad_indices[:,0], dtype = tf.int64), segment_ids = bad_indices[:,0]) - 1\n",
        "    # The number of batch_indices (this should be batch_size unless it is a partially full batch)\n",
        "    length = tf.shape(size, out_type = tf.int64)[0]\n",
        "    # Finds the cumulative sum which we can use for indexing later\n",
        "    cum = tf.cumsum(size)\n",
        "    # The offsets between each example in the batch due to our concatenation of the keys in the decode_example method\n",
        "    length_range = tf.range(start = 0, limit = length, delta = 1, dtype = tf.int64)\n",
        "    # Indices of the SparseTensor's indices member of the rows we added by the concatenation of our keys in the decode_example method\n",
        "    cum_range = cum + length_range\n",
        "\n",
        "    # The keys that we have extracted back out of our concatenated SparseTensor\n",
        "    gathered_indices = tf.squeeze(tf.gather(bad_indices, cum_range)[:,1])\n",
        "\n",
        "    # The enumerated row indices of the SparseTensor's indices member\n",
        "    sparse_indices_range = tf.range(tf.shape(bad_indices, out_type = tf.int64)[0], dtype = tf.int64)\n",
        "\n",
        "    # We want to find here the row indices of the SparseTensor's indices member that are of our actual data and not the concatenated rows\n",
        "    # So we want to find the intersection of the two sets and then take the opposite of that\n",
        "    x = sparse_indices_range\n",
        "    s = cum_range\n",
        "\n",
        "    # Number of multiples we are going to tile x, which is our sparse_indices_range\n",
        "    tile_multiples = tf.concat([tf.ones(tf.shape(tf.shape(x)), dtype=tf.int64), tf.shape(s, out_type = tf.int64)], axis = 0)\n",
        "    # Expands x, our sparse_indices_range, into a rank 2 tensor and then multiplies the rows by 1 (no copying) and the columns by the number of examples in the batch\n",
        "    x_tile = tf.tile(tf.expand_dims(x, -1), tile_multiples)\n",
        "    # Essentially a vectorized logical or, that we then negate\n",
        "    x_not_in_s = ~tf.reduce_any(tf.equal(x_tile, s), -1)\n",
        "\n",
        "    # The SparseTensor's indices that are our actual data by using the boolean_mask we just made above applied to the entire indices member of our SparseTensor\n",
        "    selected_indices = tf.boolean_mask(tensor = bad_indices, mask = x_not_in_s, axis = 0)\n",
        "    # Apply the same boolean_mask to the entire values member of our SparseTensor to get the actual values data\n",
        "    selected_values = tf.boolean_mask(tensor = bad_values, mask = x_not_in_s, axis = 0)\n",
        "\n",
        "    # Need to replace the first column of our selected_indices with keys, so we first need to tile our gathered_indices\n",
        "    tiling = tf.tile(input = tf.expand_dims(gathered_indices[0], -1), multiples = tf.expand_dims(size[0] , -1))\n",
        "    \n",
        "    # We have to repeatedly apply the tiling to each example in the batch\n",
        "    # Since it is jagged we cannot use tf.map_fn due to the stacking of the TensorArray, so we have to create our own custom version\n",
        "    def loop_body(i, tensor_grow):\n",
        "      return i + 1, tf.concat(values = [tensor_grow, tf.tile(input = tf.expand_dims(gathered_indices[i], -1), multiples = tf.expand_dims(size[i] , -1))], axis = 0)\n",
        "\n",
        "    _, result = tf.while_loop(lambda i, tensor_grow: i < length, loop_body, [tf.constant(1, dtype = tf.int64), tiling])\n",
        "    \n",
        "    # Concatenate tiled keys with the 2nd column of selected_indices\n",
        "    selected_indices_fixed = tf.concat([tf.expand_dims(result, -1), tf.expand_dims(selected_indices[:, 1], -1)], axis = 1)\n",
        "    \n",
        "    # Combine everything together back into a SparseTensor\n",
        "    remapped_sparse_tensor = tf.SparseTensor(indices = selected_indices_fixed, values = selected_values, dense_shape = sparse_tensor.dense_shape)\n",
        "    return remapped_sparse_tensor\n",
        "\n",
        "    \n",
        "  def parse_tfrecords(filename, vocab_size):\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        num_epochs = None # indefinitely\n",
        "    else:\n",
        "        num_epochs = 1 # end-of-input after this\n",
        "    \n",
        "    files = tf.gfile.Glob(os.path.join(args['input_path'], filename))\n",
        "    \n",
        "    # Create dataset from file list\n",
        "    dataset = tf.data.TFRecordDataset(files)\n",
        "    dataset = dataset.map(lambda x: decode_example(x, vocab_size))\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    dataset = dataset.batch(args['batch_size'])\n",
        "    dataset = dataset.map(lambda x: remap_keys(x))\n",
        "    return dataset.make_one_shot_iterator().get_next()\n",
        "  \n",
        "  def _input_fn():\n",
        "    features = {\n",
        "      WALSMatrixFactorization.INPUT_ROWS: parse_tfrecords('items_for_user-*-of-*', args['nitems']),\n",
        "      WALSMatrixFactorization.INPUT_COLS: parse_tfrecords('users_for_item-*-of-*', args['nusers']),\n",
        "      WALSMatrixFactorization.PROJECT_ROW: tf.constant(True)\n",
        "    }\n",
        "    return features, None\n",
        "  \n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0iNz4_O0b4j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_top_k(user, item_factors, k):\n",
        "  all_items = tf.matmul(tf.expand_dims(user, 0), tf.transpose(item_factors))\n",
        "  topk = tf.nn.top_k(all_items, k=k)\n",
        "  return tf.cast(topk.indices, dtype=tf.int64)\n",
        "    \n",
        "def batch_predict(args):\n",
        "  import numpy as np\n",
        "  \n",
        "  # read vocabulary into Python list for quick index-ed lookup\n",
        "  def create_lookup(filename):\n",
        "      from tensorflow.python.lib.io import file_io\n",
        "      dirname = os.path.join(args['input_path'], 'transform_fn/transform_fn/assets/')\n",
        "      with file_io.FileIO(os.path.join(dirname, filename), mode='r') as ifp:\n",
        "        return [x.rstrip() for x in ifp]\n",
        "  originalItemIds = create_lookup('vocab_items')\n",
        "  originalUserIds = create_lookup('vocab_users')\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
        "                         num_rows=args['nusers'], num_cols=args['nitems'],\n",
        "                         embedding_dimension=args['n_embeds'],\n",
        "                         model_dir=args['output_dir'])\n",
        "           \n",
        "    # but for in-vocab data, the row factors are already in the checkpoint\n",
        "    user_factors = tf.convert_to_tensor(estimator.get_row_factors()[0]) # (nusers, nembeds)\n",
        "    # in either case, we have to assume catalog doesn't change, so col_factors are read in\n",
        "    item_factors = tf.convert_to_tensor(estimator.get_col_factors()[0])# (nitems, nembeds)\n",
        "    \n",
        "    # for each user, find the top K items\n",
        "    topk = tf.squeeze(tf.map_fn(lambda user: find_top_k(user, item_factors, args['topk']), \n",
        "                                user_factors, dtype=tf.int64))\n",
        "    with file_io.FileIO(os.path.join(args['output_dir'], 'batch_pred.txt'), mode='w') as f:\n",
        "      for userId, best_items_for_user in enumerate(topk.eval()):\n",
        "        f.write(originalUserIds[userId] + '\\t') # write userId \\t item1,item2,item3...\n",
        "        f.write(','.join(originalItemIds[itemId] for itemId in best_items_for_user) + '\\n')\n",
        "\n",
        "def train_and_evaluate(args):\n",
        "    train_steps = int(0.5 + (1.0 * args['num_epochs'] * args['nusers']) / args['batch_size'])\n",
        "    steps_in_epoch = int(0.5 + args['nusers'] / args['batch_size'])\n",
        "    print('Will train for {} steps, evaluating once every {} steps'.format(train_steps, steps_in_epoch))\n",
        "    def experiment_fn(output_dir):\n",
        "        return tf.contrib.learn.Experiment(\n",
        "            tf.contrib.factorization.WALSMatrixFactorization(\n",
        "                         num_rows=args['nusers'], num_cols=args['nitems'],\n",
        "                         embedding_dimension=args['n_embeds'],\n",
        "                         model_dir=args['output_dir']),\n",
        "            train_input_fn=read_dataset(tf.estimator.ModeKeys.TRAIN, args),\n",
        "            eval_input_fn=read_dataset(tf.estimator.ModeKeys.EVAL, args),\n",
        "            train_steps=train_steps,\n",
        "            eval_steps=1,\n",
        "            min_eval_frequency=steps_in_epoch\n",
        "        )\n",
        "\n",
        "    from tensorflow.contrib.learn.python.learn import learn_runner\n",
        "    learn_runner.run(experiment_fn, args['output_dir'])\n",
        "    \n",
        "    batch_predict(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "12yR8dnm0b4k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('wals_trained', ignore_errors=True)\n",
        "train_and_evaluate({\n",
        "    'output_dir': 'wals_trained',\n",
        "    'input_path': 'gs://{}/wals/preproc_tft'.format(BUCKET),\n",
        "    'num_epochs': 0.05,\n",
        "    'nitems': 5668,\n",
        "    'nusers': 82802,\n",
        "\n",
        "    'batch_size': 512,\n",
        "    'n_embeds': 10,\n",
        "    'topk': 3\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNnfRHkd0b4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls wals_trained"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YA0c0-gr0b4o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head wals_trained/batch_pred.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJiZ_DB30b4q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run as a Python module\n",
        "\n",
        "Let's run it as Python module for just a few steps."
      ]
    },
    {
      "metadata": {
        "id": "3MGvO6Jt0b4s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%bash\n",
        "\n",
        "NITEMS=$(gsutil cat gs://${BUCKET}/wals/preproc_tft/nitems-00000-of-00001)\n",
        "NUSERS=$(gsutil cat gs://${BUCKET}/wals/preproc_tft/nusers-00000-of-00001)\n",
        "\n",
        "rm -rf wals_trained\n",
        "export PYTHONPATH=${PYTHONPATH}:${PWD}/wals_tft\n",
        "python -m trainer.task \\\n",
        "   --output_dir=${PWD}/wals_trained \\\n",
        "   --input_path=gs://${BUCKET}/wals/preproc_tft \\\n",
        "   --num_epochs=0.01 --nitems=$NITEMS --nusers=$NUSERS \\\n",
        "   --job-dir=./tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "av0pcVfT0b4u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run on Cloud"
      ]
    },
    {
      "metadata": {
        "id": "7qq_8xQD0b4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%bash\n",
        "OUTDIR=gs://${BUCKET}/wals_tft/model_trained\n",
        "JOBNAME=wals_$(date -u +%y%m%d_%H%M%S)\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "\n",
        "NITEMS=$(gsutil cat gs://${BUCKET}/wals/preproc_tft/nitems-00000-of-00001)\n",
        "NUSERS=$(gsutil cat gs://${BUCKET}/wals/preproc_tft/nusers-00000-of-00001)\n",
        "\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=${PWD}/wals_tft/trainer \\\n",
        "   --job-dir=$OUTDIR \\\n",
        "   --staging-bucket=gs://$BUCKET \\\n",
        "   --scale-tier=BASIC_GPU \\\n",
        "   --runtime-version=1.6 \\\n",
        "   -- \\\n",
        "   --output_dir=$OUTDIR \\\n",
        "   --input_path=gs://${BUCKET}/wals/preproc_tft \\\n",
        "   --num_epochs=10 --nitems=$NITEMS --nusers=$NUSERS "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SnqPmrER0b4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This took <b>10 minutes</b> for me."
      ]
    },
    {
      "metadata": {
        "id": "SfOUtczk0b4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get row and column factors\n",
        "\n",
        "Once you have a trained WALS model, you can get row and column factors (user and item embeddings) from the checkpoint file. We'll look at how to use these in the section on building a recommendation system using deep neural networks."
      ]
    },
    {
      "metadata": {
        "id": "ACxSuDkW0b4x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_factors(args):\n",
        "  with tf.Session() as sess:\n",
        "    estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
        "                         num_rows=args['nusers'], num_cols=args['nitems'],\n",
        "                         embedding_dimension=args['n_embeds'],\n",
        "                         model_dir=args['output_dir'])\n",
        "    row_factors = estimator.get_row_factors()[0]\n",
        "    col_factors = estimator.get_col_factors()[0]\n",
        "    return row_factors, col_factors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1cqTpUcN0b4y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'output_dir': 'gs://{}/wals_tft/model_trained'.format(BUCKET),\n",
        "    'nitems': 5668,\n",
        "    'nusers': 82802,\n",
        "    'n_embeds': 10\n",
        "  }\n",
        "\n",
        "user_embeddings, item_embeddings = get_factors(args)\n",
        "print user_embeddings[:3]\n",
        "print item_embeddings[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r1H0kUDQ0b40",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can visualize the embedding vectors using dimensional reduction techniques such as PCA."
      ]
    },
    {
      "metadata": {
        "id": "06Sd7rVF0b40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(user_embeddings)\n",
        "user_embeddings_pca = pca.transform(user_embeddings)\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "xs, ys, zs = user_embeddings_pca[::150].T\n",
        "ax.scatter(xs, ys, zs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-UQhbZJX0b42",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<pre>\n",
        "# Copyright 2018 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "</pre>"
      ]
    },
    {
      "metadata": {
        "id": "svOYU4uS0b42",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}