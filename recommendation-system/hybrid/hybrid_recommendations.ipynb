{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network hybrid recommendation system on Google Analytics data model and training\n",
    "\n",
    "This notebook demonstrates how to implement a hybrid recommendation system using a neural network to combine content-based and collaborative filtering recommendation models using Google Analytics data. We are going to use the learned user embeddings from [wals.ipynb](../wals.ipynb) and combine that with our previous content-based features from [content_based_using_neural_networks.ipynb](../content_based_using_neural_networks.ipynb)\n",
    "\n",
    "Now that we have our data preprocessed from BigQuery and Cloud Dataflow, we can build our neural network hybrid recommendation model to our preprocessed data. Then we can train locally to make sure everything works and then use the power of Google Cloud ML Engine to scale it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use TensorFlow Hub to use trained text embeddings, so let's first pip install that and reset our session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/f0/3a3ced04c8359e562f1b91918d9bde797c8a916fcfeddc8dc5d673d1be20/tensorflow_hub-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (1.10.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.4.0->tensorflow_hub) (40.2.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reset the notebook's session kernel! Since we're no longer using Cloud Dataflow, we'll be using the python3 kernel from here on out so don't forget to change the kernel if it's still python2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0308 16:58:05.404504 139879471625984 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "# Import helpful libraries and setup our project, bucket, and region\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "PROJECT = 'qwiklabs-gcp-4a684069c4776675'\n",
    "BUCKET = 'colaborative-filtering-agea'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "NUMBER_OF_DAYS=5\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_query(number_of_days, eval):\n",
    "  query = \"\"\"\n",
    "    SELECT \n",
    "     h.*\n",
    "    FROM `AGEA_ASL.Dataset_Hybrid` h\n",
    "    JOIN `AGEA_ASL.Dataset_A`a ON h.content_id = a.content_id\n",
    "    WHERE date_diff(CAST('2018-12-31' as DATE),CAST(a.day_write as DATE), day) < {}\n",
    "  \"\"\".format(number_of_days)\n",
    "  \n",
    "  if(eval):\n",
    "    query+= \"AND ABS(MOD(h.hash_id,10)) < 3\"\n",
    "  else:\n",
    "    query+= \"AND ABS(MOD(h.hash_id,10)) >= 3\"\n",
    "  return query\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hybrid_data(number_of_days, eval):\n",
    "  query = create_hybrid_query(number_of_days,eval)\n",
    "  return bq.Query(query).execute().result().to_dataframe()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_eval_data = get_hybrid_data(NUMBER_OF_DAYS, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_eval_data.to_csv('data/hybrid_dataset_eval.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_training_data = get_hybrid_data(NUMBER_OF_DAYS, eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_training_data.to_csv('data/hybrid_dataset_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,content_id,title,section_1,tag_1,d2v,user_factors,item_factors,hash_id,next_article,doc2vec_avg,gender,age\r\n",
      "6684642,uU9fSN3rZ,Prioridad para Mauricio Macri: evitar cualquier disparada del dólar,Opinión,Política económica,-0.5431375503540038|1.5081772804260252|0.28818225860595703|-0.22279983758926392|-0.75957942008972157|1.6420205831527708|0.017720999196171761|-1.839012503623962|-0.17818327248096469|-0.58572870492935181|-1.1695746183395384|-2.074692964553833|0.84690624475479115|-0.227681428194046|1.3209612369537351|-0.11642815172672273|0.031647033989429481|-1.0423736572265625|0.98129534721374545|-0.27149784564971924|-0.73087340593338013|-1.3783644437789917|0.65492689609527588|1.7048561573028562|1.3716107606887815|0.26091566681861877|-0.72071701288223255|-2.7966279983520508|1.5285329818725584|2.1016929149627686|-0.75358551740646373|-1.2115024328231809|-0.46927991509437561|-0.96890002489089944|0.478855550289154|2.7522444725036621e-05|-0.934307873249054|1.2002404928207395|-0.076965652406215682|1.4630620479583742|-0.80288654565811146|0.21216972172260284|-0.98924684524536144|-0.1133676394820213|2.2153763771057129|-0.02780250646173954|0.95878875255584717|1.979474663734436|1.0394277572631838|-0.71349042654037476,0.00050430687|0.0015023915|0.003427962|-0.0014987885|-0.009394963|0.00025551338|-0.007554945|-0.0072529493|0.000551092|0.0040260768,0.0005537326|-0.0015965832|0.00082380266|-0.0030222572|-0.002081631|3.0389727e-05|-2.8769919e-05|0.000760394|0.0023760488|-0.0029913778,-3208869835700753475,nainTnpx7,0.53977033805755348|0.38040443953353958|0.048109383982169261|0.079343121874000458|0.34391660345894831|-0.22250054430705382|0.10269956401509556|-0.69835061161730294|0.522016603696067|0.46944862538154186|-0.41608799686677483|-0.43247724314588826|-0.064944593851030824|0.083986915711432644|0.29011047554811503|-0.53475425615840511|-0.450126029996828|-0.42889403309662028|0.88774420599943049|-0.069734703792453265|-0.11638198292127833|-0.38635338763088439|0.44913751834901983|0.84433749833881333|0.4777152030287371|-0.084435692235774518|-0.53388220313837131|-1.2402859897831655|-0.017766866975318266|0.96244160663689582|-0.53423980709222685|-0.12104865027618077|-0.23415612046865047|-0.13367427022734851|0.60407006817371034|0.28216035669764417|-0.42230063787566596|0.038091793218274467|-0.099719592566509238|-0.26220666605899751|0.35425858960102929|0.40211057362822011|0.64252496197691678|0.25377532914479523|0.23630886851540683|-0.13906528364196072|0.0075175844759667191|0.601872243841236|-0.34612926322320181|-0.34954348722857903,Masculino,66\r\n",
      "4236150,pvfv0QLrC,Venden 12 sucursales de las heladerías Freddo en $ 55 millones,Economía - Economía,Gastronomía,0.56656444072723389|0.37202057242393483|1.3455761671066286|0.36593317985534662|0.47776028513908392|-1.1519147157669067|-0.93041497468948364|-0.18333987891674042|0.18208913505077359|0.73227638006210316|-1.0327811241149902|-0.6714860200881958|-0.28511294722557068|-0.46011188626289368|-0.020507313311100006|-0.087610490620136247|-0.4903934001922608|-1.0280747413635252|0.52220290899276733|-0.33938381075859064|0.21743600070476526|0.26295033097267156|0.074561595916748033|1.0790762901306152|0.63540798425674438|1.1935672760009763|-0.7580302357673645|0.4865572452545166|0.81076753139495861|1.5624737739562988|-0.82558131217956543|0.24198675155639651|-0.56575065851211548|-1.476050496101379|1.1577333211898804|0.993734657764435|-0.13660404086112976|0.33524563908576965|-0.73199605941772472|0.92364591360092163|-0.461458832025528|0.683887779712677|-0.14382821321487427|0.47788363695144642|-1.3910535573959351|-0.10642120242118835|0.2687241137027741|0.087688848376274109|-0.88320064544677734|-0.41671097278594971,0.00042228177|-0.0004191149|-0.0023930953|-0.00057022175|-0.00042613465|0.0006306297|-0.000490456|-0.0008334862|0.0018969057|0.001146943,-0.0054745786|-0.00022036681|0.0032007694|-0.0028230143|0.0019302719|0.0018610728|-0.0053485665|-0.004456335|0.0047272984|-9.123652e-05,8157253981277868088,NFYK36lZM,0.3697971118698743|-0.074440923773429618|0.60172981413250615|0.29235621507872228|0.58148619430986315|-0.310258491303433|0.169434611431577|-0.19919160245494402|0.46162238581614057|0.79254606840285391|-0.67474833164702752|-0.28728017845953058|-0.066055216403170067|0.14205275916240423|-0.10709544245830993|-0.79866350069642067|-0.28019989315759047|-0.63335900923067867|0.680482727814127|0.56441764389588078|-0.31859343761408876|-0.13985247148031535|0.53750379163433193|0.66112719154493371|0.58683853460983793|0.36564174675467337|-0.74621532213958819|-0.95388394187797187|0.071281472898342413|0.4798559573563661|-0.20099287293851373|-0.23249640586701306|0.21049589816142214|0.085845136506990979|0.666966636749831|0.44289882887493481|0.00015453791076486256|-0.70557404351844011|-0.0478299158540639|-0.088346147232434974|0.4804220639846542|0.908118595115163|0.40438739043152472|0.43729581599208467|0.08022426291030238|-0.08170933479612523|0.63337010187520226|0.2108782760121605|-0.38433547728610312|-0.776792831041596,Masculino,59\r\n",
      "4535122,H96jQUNxP,Detuvieron al ex presidente de Lotería en el gobierno de Daniel Scioli,Política,Daniel Scioli,0.38895833492279047|1.6486941576004031|0.12129914760589597|0.25228807330131525|-0.36573502421379084|-0.63559496402740479|-0.020904982462525368|0.193623885512352|2.1354782581329346|0.054335772991180413|-1.1197347640991211|0.63700407743453979|-0.25332453846931458|1.1944018602371216|1.1640568971633911|0.10441001504659651|-0.52509969472885132|-2.2114267349243164|2.2724294662475586|-0.51995724439620972|1.8721370697021489|-0.82199960947036743|2.2053127288818359|0.25744777917861933|0.68737781047821045|0.88322550058364857|-0.4796161949634552|-0.69549757242202759|-0.42504629492759693|0.79142707586288452|0.16637147963047028|0.34353768825531|-0.67268663644790638|-1.2571989297866819|1.2295110225677488|-0.34959301352500916|-0.1126171573996544|-1.1961950063705444|0.93576729297637962|-0.60554903745651245|-0.29021602869033808|0.43398869037628163|0.45509636402130121|0.54054760932922363|1.1398333311080933|1.384873628616333|0.26996132731437678|0.20800034701824191|0.50989741086959839|-0.11488819867372513,-0.0007977136|0.00087002263|-0.000543138|0.0012167797|8.68129e-05|0.003910606|6.5631335e-05|-0.0014228629|1.1510528e-05|0.00083667034,0.00091878977|0.0014115523|0.00014006016|-0.005746135|-0.000715595|-0.0012999182|-0.0019979016|-0.00045602297|-1.9051933e-05|-0.0014268246,2053905988453338304,eP4Mi8J9V,0.55642846971750259|0.57680398013649714|0.04078566593428451|0.049456013444190226|0.4493837782906161|-0.13381532310611666|-0.17948895578997|-0.16480137925181124|0.41528000517023939|0.55713678958515311|-0.53227935048441088|-0.040988788526091342|0.011420334792799455|0.12105431159337363|0.13563198844591767|-0.8757396973669529|-0.68418911678923489|-0.84138404991891635|0.75138658704236139|-0.086984344447652517|-0.067463486041459769|-0.048129388441642107|0.41429115997420418|0.42266574864172274|0.16845844913687974|0.045201523436440356|-0.48154633906152505|-0.96311996297703839|-0.060969805551899785|0.61793797463178668|-0.68417325740059209|0.24333951084150207|-0.27961470393670934|-0.18120510193208861|0.602186611543099|0.41680415471394855|-0.22981352752281564|0.10359665316840011|-0.13221352350794605|-0.634378881090217|0.60754856994996453|0.40824640945841867|0.72866281535890354|0.47418727311823111|0.0440784407158693|-0.13099364211989778|0.48063063719827287|0.43369976306955021|-0.5201152041554451|-0.60055136804779352,Masculino,63\r\n",
      "5837462,wGDr9sbr8,Llega al país una nueva marca de nafta,Economía - Empresas y Negocios,Aumento de la nafta,0.79907220602035522|0.37159013748168956|0.367965042591095|0.21244241297245026|0.30140680074691772|-0.60631251335144043|0.22303944826126096|-0.36384963989257807|0.37896630167961121|0.76708412170410145|-0.59143054485321045|-0.53329592943191528|0.0079677347093820555|0.19093161821365359|0.079635016620159163|0.348349392414093|-0.9591946005821228|-0.79750782251358032|0.36763864755630488|0.45275503396987921|0.69066470861434925|-1.0188126564025879|0.36133340001106257|0.99940919876098644|0.4707661271095277|0.52231431007385254|-0.14863656461238861|-0.6617845892906189|0.54337090253829956|1.5314950942993164|-1.241300106048584|0.65700012445449829|-0.29139050841331482|-1.3434032201766968|0.75818508863449086|0.14523012936115265|-1.1026064157485962|0.59331238269805908|-0.64461863040924072|-1.1610201597213743|0.090799860656261444|1.5030525922775269|-0.73316007852554321|0.74869871139526367|-1.0427531003952026|-0.50705116987228382|0.68363451957702637|0.49026903510093689|-0.13935472071170807|-1.0722453594207764,-0.005148123|0.0092768455|0.00022138223|0.0019355172|0.000285859|-0.0022306852|0.003976915|-0.0032495994|0.0030740807|-0.005594331,-9.957495e-05|-0.00027400223|0.0012946072|-0.0013046805|-0.0015004362|-0.009669154|0.0027248785|-0.00033452513|0.0056263343|-0.003768002,-1003265028361963784,iJiZLqWtI,0.77466630115032342|0.12477410234101306|0.19992814951269111|-0.14278241156732072|0.60056064938102416|-0.24777706541823166|-0.43064907691514615|-0.22187761014756505|0.61055477869974839|0.60413620690205627|-0.44438163479741316|-0.084691524249623465|0.3574572221293108|0.12882204162181202|-0.024369428626685868|-0.84691415899750122|-0.79519564175675994|-0.52849603355594033|0.46977217233842189|0.025467383309574643|-0.16033672245892244|-0.26049178755105873|0.265832363062231|0.64389810871079312|0.046189134350504854|-0.29244815148120618|-0.65534117769636424|-1.2110071742799895|-0.10782556682191201|0.24059640081155637|-0.31558699507987931|0.22153801837794637|-0.45546414075585095|0.44753518189835073|0.861293931501425|0.67315295455655511|-0.43080301813144928|-0.33519733787870215|-0.44365736067841194|-0.11639942840248815|0.80136230875645309|0.58017820241758622|0.74694192511424484|0.3505909501755346|-0.014424436208816397|-0.15727375814305983|0.11004833566970845|0.35603481415093569|-0.32704791594394228|-0.76236272923994552,Femenino,31\r\n",
      "7115275,7U-NDcQB0,\"Luka Doncic, el joven maravilla que sonríe mientras hace estragos en la NBA\",Deportes - Básquet,NBA,0.970485508441925|1.2161774635314941|0.23553913831710815|-0.38183182477951044|0.99418008327484142|0.23593018949031824|-0.67685079574584961|2.3017046451568604|-0.49065506458282482|-0.14486406743526459|-1.2627120018005373|0.63900154829025269|-1.576728820800781|0.62174355983734131|-1.2367430925369263|-1.9341793060302732|-1.2530959844589231|1.6738585233688357|0.38843145966529852|0.68210834264755249|-2.91720986366272|0.31838899850845342|0.651725172996521|-0.63759857416152965|1.3158458471298216|1.6417317390441895|0.61669856309890736|0.59998685121536255|0.42905968427658081|-0.15661370754241946|-1.9926347732543943|0.28452527523040771|-0.1866271048784256|-1.0292322635650635|-0.48376339673995972|1.3721132278442385|2.6071462631225586|0.35799247026443481|-0.19200366735458371|-1.2434318065643311|0.85209590196609475|0.65408939123153687|1.4038223028182983|0.89350128173828125|2.561923742294312|-0.7478315830230714|2.1109695434570312|0.33620280027389532|-1.5438650846481325|-1.5870758295059202,-0.0012159231|0.00062846806|0.0010169491|0.00064580375|-0.0010831147|0.0027635044|0.0025699255|0.0016739137|0.0018028655|0.00047603901,-0.0024407362|-0.00044378184|0.0023735214|-0.00543591|0.0020092062|0.00794174|-0.004813322|-0.0032812872|0.0026576533|-0.0038023642,7974908734352204379,8eJKBnNSq,0.88487594308597683|0.43788180846188751|0.14491191739216447|-0.14418479181560023|0.85698619591338276|-0.31856476735057565|-0.3788636436552873|0.051544846700770468|0.47256760528710273|0.54600443770842888|-0.35664697137794321|0.17750914107143348|0.11345346737653018|0.4735863286270095|0.14043713520680151|-0.90338031308991573|-0.61953654472849207|-0.13603144977241755|0.48331965951781186|0.28394239562164453|-0.14948633326483624|0.10720643600715059|0.021532390000564713|0.728172698191234|0.10955418001062103|-0.11821686409946|-0.66042085736989975|-1.2974124018635069|-0.00824120441185575|0.38073008028524269|-0.42419151083699291|0.34154089487024719|-0.43599541724792545|0.11643937549420766|0.632395408116281|0.632745980152062|-0.33609375582974665|-0.0671992476231286|-0.4354585253500512|-0.16063444865202259|0.80497058095144369|0.23512870565588984|0.70578646101057541|0.43978212521012339|0.46844158055526869|-0.29388396421979573|0.561718731519899|0.34020280881252662|-0.568713259949748|-1.2232613103198153,Femenino,43\r\n",
      "3888948,SZS09KtDg,Tragedia en un country: atropelló y mató a su empleada cuando sacaba el auto del garaje,Ciudades,Inseguridad  vial,0.67810428142547607|0.030975332483649257|1.2279855012893677|0.1904016584157944|0.64147937297821045|-0.50275874137878407|-0.26219233870506287|-0.58860999345779419|0.71425014734268188|0.4668411910533905|-0.78249692916870117|-0.042077142745256417|0.36686623096466059|0.024646705016493797|0.15533174574375153|-0.68005377054214478|-1.133289098739624|-0.5900699496269225|0.942360281944275|-0.15692038834095|-0.37032669782638555|-0.012568719685077667|0.95671677589416515|0.41072756052017212|0.48351293802261353|0.10879501700401305|-0.77244198322296143|-1.2191401720046997|0.25025320053100586|0.964466869831085|-0.94907498359680165|-0.16119028627872467|-0.38863226771354681|0.290568470954895|0.91002416610717762|0.62191575765609741|0.31249544024467468|-0.68083775043487538|-0.26702591776847839|-0.80371731519699074|0.85713106393814076|0.8315936326980593|0.51093947887420665|0.14183039963245392|-0.030848318710923195|-0.15116180479526523|0.30523544549942017|0.75024086236953735|-0.97193664312362682|-0.32422676682472229,-0.0027792596|0.0019437156|0.0011267769|0.0010752933|0.0005219877|-0.0016052397|0.0015690959|0.0004890986|0.0015186078|-0.0014024393,0.0013357894|-0.0031251803|-0.0010338216|0.00026631297|-0.0013558638|-0.0026412415|0.001466639|-0.0005495111|0.0054079797|-0.0024653845,-4136705496955150166,pJ3rJPH1F,1.09998501580337|0.17383101248535612|0.05121458930380899|-0.18728762789597286|0.45498234399690696|-0.36724520031491226|-0.91231013885859791|-0.17765016914827042|0.40872505872414022|0.68452312822999628|-0.3739499446839607|-0.059880807803108754|0.35668573997400954|-0.029867945415577968|-0.26845231742180631|-0.65605168860277208|-0.80105766679706258|-0.0487781336595272|0.55462332506631984|-0.019071694591949708|-0.65468202044801982|-0.189408995468041|0.13491995594110978|0.547473652481005|0.1049621254205704|-0.031550356283270076|-0.22601136308291858|-0.927844141064019|-0.13442567311998072|-0.042866895553366889|-0.59970449450714824|0.1656188076554701|-0.54108836694524209|0.19338692262254911|0.680343536595846|0.964940158200675|-0.3018633291237699|-0.11104146018624303|-0.48011319872377245|0.01510241431794291|0.838255341453799|0.52291614228281469|0.65470485864528283|0.21173583963436302|0.0611460305878828|0.019129804014388851|0.81202213646009058|0.24341652580890163|-0.21808206508385725|-1.2658192299563311,Femenino,61\r\n",
      "5537929,SZS09KtDg,Tragedia en un country: atropelló y mató a su empleada cuando sacaba el auto del garaje,Ciudades,Inseguridad  vial,0.67810428142547607|0.030975332483649257|1.2279855012893677|0.1904016584157944|0.64147937297821045|-0.50275874137878407|-0.26219233870506287|-0.58860999345779419|0.71425014734268188|0.4668411910533905|-0.78249692916870117|-0.042077142745256417|0.36686623096466059|0.024646705016493797|0.15533174574375153|-0.68005377054214478|-1.133289098739624|-0.5900699496269225|0.942360281944275|-0.15692038834095|-0.37032669782638555|-0.012568719685077667|0.95671677589416515|0.41072756052017212|0.48351293802261353|0.10879501700401305|-0.77244198322296143|-1.2191401720046997|0.25025320053100586|0.964466869831085|-0.94907498359680165|-0.16119028627872467|-0.38863226771354681|0.290568470954895|0.91002416610717762|0.62191575765609741|0.31249544024467468|-0.68083775043487538|-0.26702591776847839|-0.80371731519699074|0.85713106393814076|0.8315936326980593|0.51093947887420665|0.14183039963245392|-0.030848318710923195|-0.15116180479526523|0.30523544549942017|0.75024086236953735|-0.97193664312362682|-0.32422676682472229,-0.0011112575|0.0015062314|-0.0011002064|0.0018593341|-0.0025292865|0.0046923403|0.003503973|-0.004150534|0.007227553|-0.006692175,0.0013357894|-0.0031251803|-0.0010338216|0.00026631297|-0.0013558638|-0.0026412415|0.001466639|-0.0005495111|0.0054079797|-0.0024653845,6150737599713808519,ESs_2rxB2,0.71384854722528779|0.18350980324542823|0.28317649359462943|-0.1553947026583293|0.52875110587475815|-0.3859740994830782|-0.33548692273860437|-0.087960738733279475|0.58168444902195127|0.46194804633641473|-0.44761104045684447|-0.02302783310574056|0.16142517915617213|0.26570753185242713|0.14822650106590934|-0.92606111563582638|-0.69728131653032621|-0.46669632156311486|0.61258278088292251|-0.024353927791925613|-0.28747250752873066|-0.1497498922476955|0.311974698968404|0.58849648832430335|0.050878329195495078|-0.2349877674941922|-0.72270821276360075|-1.0908885481567956|0.033811763040979465|0.38634565405744675|-0.3493267946573963|0.25135899613369761|-0.36679622656061656|0.32380915127673487|0.75108062404604348|0.60231199846497374|-0.19150268984137464|-0.28682961875821461|-0.39202524207401024|-0.19435725342605481|0.70237722144502057|0.5987306353178633|0.67853332896217189|0.40177643525599488|0.063507810447327548|-0.17942956771985427|0.20172678214270304|0.43827544346038866|-0.43739699930837017|-0.7613180848254919,Femenino,40\r\n",
      "7003595,WybrcwZoq,Venezuela se despide del 2018 prendiendo fuego todas sus calamidades,Mundo,Venezuela,-0.57994139194488525|0.35702854394912714|-0.86256432533264149|-0.081804901361465468|-0.36320263147354126|-0.84362691640853893|0.79705530405044545|-0.43411266803741455|-0.31993192434310919|1.6180336475372314|0.62568336725234985|-0.045867457985877991|1.1537789106369019|-1.5567599534988403|0.44283348321914673|-0.22712096571922305|0.62613373994827282|0.24628508090972895|-0.090087220072746277|0.9305930733680724|0.1662021279335022|-0.75730365514755249|0.55388653278350841|1.5102071762084961|1.2608801126480103|1.5167922973632815|-1.0625187158584595|-0.99153417348861683|0.19691191613674164|0.27114763855934143|0.041068796068429947|-0.21384298801422116|0.081473588943481445|-0.843574285507202|1.0411162376403809|0.45997437834739691|-0.71129214763641357|-0.38168221712112421|-0.61211472749710083|-0.76701462268829335|-0.62141686677932739|0.81750136613845825|0.10350169241428377|-0.31901535391807562|-1.2793474197387695|-1.2057750225067139|0.39190450310707087|0.0808190405368805|-0.071033373475074768|-0.74682605266571034,-0.0023272776|0.0008297236|-0.0007023355|0.00054722757|-0.00023266222|0.0034577658|-0.00040552698|-0.0026317125|-0.0015217937|0.0010261425,0.0017589676|-0.00011345065|-9.923019e-05|-0.0008923377|-0.0031619514|0.0015352925|0.0018700483|-0.00019679914|0.0005672904|6.758713e-05,-150913621953179983,HzNYkuIKN,-0.18735354640247187|0.44372519500115337|0.24039848636397543|0.054415422601296635|-0.0055554420413339786|-0.142666393178789|0.14415078973123696|-0.36996827696395274|0.38540280141922478|1.1218541705542626|-0.546210774604012|-0.46049097629592689|-0.10911573579206187|-0.19950950090937752|-0.13854361259761977|-0.58976327623788483|-0.5256832353770734|-0.40524403219494748|0.53987899236381054|0.1906122293542413|-0.48391601442605908|-0.33797471959363007|0.35123312144595031|0.6295598862583146|0.71806904375005287|0.49759358058080966|-0.36337999780388436|-1.2904477677143671|0.14385145986178785|1.0942419044235172|-0.73196763852063351|-0.41812157104997072|-0.11464191984166117|-0.563461181378978|0.42223780939136341|0.33815765928696179|-0.45183763855739562|0.50996722938383321|-0.33716938200899776|-0.36402715983636241|0.29445644035277996|0.851195828660446|0.32244543928433866|0.33225313858950845|-0.33311957449597474|-0.32601896821356874|0.383131434867049|0.42376785873271089|-0.13965321918401646|-0.18838208864497788,Masculino,26\r\n",
      "5158000,ZQnL9ZkT_,Estados Unidos está dividido entre la indignación y la alegría por este beso,Mundo,homosexual,0.255279928445816|-0.66474127769470215|0.16027337312698364|-1.2265700101852417|1.2338634729385378|-1.2029876708984375|-1.3572492599487305|-1.5477005243301392|-0.7559630274772644|0.81154942512512185|-0.66734296083450317|-0.772582769393921|-0.072695352137088762|-0.79485261440277111|-0.33473294973373413|-0.13422977924346927|-0.61589765548706055|-0.585115373134613|0.33526283502578735|1.4504911899566648|-0.16754673421382904|-0.098978929221630069|-0.67033863067626964|1.7318342924118042|-0.46616178750991821|-0.09544659405946733|-0.40760084986686718|-1.2622462511062622|0.4920787513256073|-0.191595122218132|0.2057989835739136|0.37721934914588928|0.02105298638343811|1.3196148872375488|0.89305686950683583|0.906208336353302|0.59806674718856812|-0.77460330724716175|-1.0470008850097656|-0.5618237853050233|1.2986834049224851|1.0671186447143555|1.0274103879928589|0.20099651813507077|-0.19271241128444672|-0.5442243218421936|1.2532296180725095|-0.3483454287052154|0.11808107048273085|-0.75390350818634044,-0.0013655962|0.00042400768|0.0011125219|0.0044553042|-0.0021777677|0.0008534397|0.0030554465|0.00028141434|0.0041035763|-0.003428803,-0.002114942|-0.00047833423|-0.0044873087|0.00018365128|-0.0018714538|-0.00093250256|-0.0023068038|0.0011975892|0.0028384048|-0.0023959996,-6445739067767783384,drfISoROd,0.94511700417206135|0.14024595881346613|0.26859275026799878|-0.020132138335611685|0.47778866149019461|-0.20887502536061223|-0.63564925416721951|-0.10889388294890526|0.62613542446342763|0.5067971528333145|-0.35317295434651896|0.0448780949227512|0.41339042940671789|0.307914994751627|0.074898734907037634|-0.84502614947268739|-0.82109245201718306|-0.39122590712213418|0.37105837343551673|-0.14566481660585859|-0.26845785405021155|-0.28988613400724716|0.29608036321587861|0.8907250874908641|-0.076535911997780218|-0.34974480810342357|-0.75026650575455289|-1.2981838260311636|-0.0276600226861774|0.21100115802255454|-0.39364906528498977|0.38326914754361491|-0.5520432915946003|0.63467130565550178|1.0076992858084848|0.9245705842040477|-0.17158663715963482|-0.58360669310422975|-0.5794837902794826|0.08775959546619562|0.78918448428157728|0.55868113850010537|0.71361216020886786|0.33705523728713166|-0.046870220779965159|-0.062400284688919741|0.034222556263557649|0.3952707618009299|-0.19316509035706991|-0.83967195288278162,Femenino,66\r\n"
     ]
    }
   ],
   "source": [
    "!head data/hybrid_dataset_train.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/hybrid_dataset_eval.csv [Content-Type=text/csv]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file://data/hybrid_dataset_train.csv [Content-Type=text/csv]...         \n",
      "/ [2 files][  3.4 GiB/  3.4 GiB]   36.1 MiB/s                                   \n",
      "Operation completed over 2 objects/3.4 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/hybrid_dataset_*.csv gs://{BUCKET}/hybrid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distinct_values(column_name, eval=False):\n",
    "  query = \"\"\"\n",
    "    SELECT COUNT(DISTINCT {}) FROM \n",
    "      ({})\n",
    "  \"\"\".format(column_name, create_hybrid_query(NUMBER_OF_DAYS,eval))\n",
    "  return bq.Query(query).execute().result().to_dataframe()['f0_'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_content_ids = count_distinct_values('content_id',eval)\n",
    "number_of_sections = count_distinct_values('section_1',eval)\n",
    "number_of_tags = count_distinct_values('tag_1',eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_values(column_name,eval=False):\n",
    "  query = \"\"\"\n",
    "    SELECT DISTINCT {} FROM \n",
    "      ({})\n",
    "  \"\"\".format(column_name, create_hybrid_query(NUMBER_OF_DAYS,eval))\n",
    "  return bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_vocab = get_column_values('section_1')['section_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_id_vocab = get_column_values('content_id')['content_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_id_vocab.to_csv('data/content_id_vocab',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create hybrid recommendation system model using TensorFlow </h2>\n",
    "\n",
    "Now that we've created our training and evaluation input files as well as our categorical feature vocabulary files, we can create our TensorFlow hybrid recommendation system model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get some of our aggregate information that we will use in the model from some of our preprocessed files we saved in Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.lib.io import file_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV and label columns\n",
    "CSV_COLUMNS = 'user_id,content_id,title,section_1,tag_1,d2v,user_factors,items_factors,next_article,doc2vec_avg,hash_id,gender,age'.split(',')\n",
    "LABEL_COLUMN = 'next_content_id'\n",
    "NON_FEATURES_COLUMNS = ['user_id','content_id','hash_id']\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [[\"Unknown\"]*len(CSV_COLUMNS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input function for training and evaluation to read from our preprocessed CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input function for train and eval\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "    def decode_csv(value_column):\n",
    "      columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
    "      features = dict(zip(COLUMNS, columns))          \n",
    "      label = features.pop(LABEL_COLUMN)\n",
    "      for non_feature in NON_FEATURES_COLUMNS:\n",
    "            features.pop(non_feature)\n",
    "      return features, label\n",
    "\n",
    "    # Create list of files that match pattern\n",
    "    file_list = tf.gfile.Glob(filename = filename)\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      num_epochs = None # indefinitely\n",
    "      dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "    else:\n",
    "      num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create our feature columns using our read in features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature columns to be used in model\n",
    "def create_feature_columns(args):\n",
    "  # Create content_id feature column\n",
    "  content_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key = \"content_id\",\n",
    "    hash_bucket_size = number_of_content_ids)\n",
    "\n",
    "  # Embed content id into a lower dimensional representation\n",
    "  embedded_content_column = tf.feature_column.embedding_column(\n",
    "    categorical_column = content_id_column,\n",
    "    dimension = args['content_id_embedding_dimensions'])\n",
    "\n",
    "  # Create section feature column\n",
    "  categorical_section_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key = \"section_1\",\n",
    "    vocabulary_list=sections_vocab,\n",
    "    num_oov_buckets = 1)\n",
    "\n",
    "  # Convert section category column into indicator column so that it can be used in a DNN\n",
    "  indicator_section_column = tf.feature_column.indicator_column(categorical_column = categorical_section_column)\n",
    "\n",
    "  # Create title feature column using TF Hub\n",
    "  embedded_title_column = hub.text_embedding_column(\n",
    "    key = \"title\", \n",
    "    module_spec = \"https://tfhub.dev/google/nnlm-es-dim50-with-normalization/1\",\n",
    "    trainable = False)\n",
    "\n",
    "  # Create tag feature column\n",
    "  tag_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key = \"tag_1\",\n",
    "    hash_bucket_size = number_of_tags + 1)\n",
    "\n",
    "  # Embed tag into a lower dimensional representation\n",
    "  embedded_tag_column = tf.feature_column.embedding_column(\n",
    "    categorical_column = tag_column,\n",
    "    dimension = args['tag_embedding_dimensions'])\n",
    "\n",
    "  # Create months since epoch boundaries list for our binning\n",
    "  age_boundaries = list(range(15, 100))\n",
    "\n",
    "  # Create age feature column using raw data\n",
    "  age_column = tf.feature_column.numeric_column(\n",
    "    key = \"age\")\n",
    "\n",
    "  # Create bucketized age feature column using our boundaries\n",
    "  age_bucketized = tf.feature_column.bucketized_column(\n",
    "    source_column = age_column,\n",
    "    boundaries = age_boundaries)\n",
    "\n",
    "  # Cross our categorical section column and bucketized age column\n",
    "  crossed_age_since_section_column = tf.feature_column.crossed_column(\n",
    "    keys = [categorical_section_column, age_bucketized],\n",
    "    hash_bucket_size = len(age_boundaries) * (number_of_sections + 1))\n",
    "\n",
    "  # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
    "  indicator_crossed_age_since_section_column = tf.feature_column.indicator_column(categorical_column = crossed_age_since_section_column)\n",
    "  \n",
    "  # Create list of feature columns\n",
    "  feature_columns = [embedded_content_column,\n",
    "                     embedded_tag_column,\n",
    "                     indicator_section_column,\n",
    "                     embedded_title_column,\n",
    "                     crossed_age_since_section_column] #+ user_factors + item_factors + d2v + d2v_avg\n",
    "\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom model function for our custom estimator\n",
    "def model_fn(features, labels, mode, params):\n",
    "  # Create neural network input layer using our feature columns defined above\n",
    "  net = tf.feature_column.input_layer(features = features)\n",
    "\n",
    "  # Create hidden layers by looping through hidden unit list\n",
    "  for units in params['hidden_units']:\n",
    "    net = tf.layers.dense(inputs = net, units = units, activation = tf.nn.relu)\n",
    "\n",
    "  # Compute logits (1 per class) using the output of our last hidden layer\n",
    "  logits = tf.layers.dense(inputs = net, units = params['n_classes'], activation = None)\n",
    "\n",
    "  # Find the predicted class indices based on the highest logit (which will result in the highest probability)\n",
    "  predicted_classes = tf.argmax(input = logits, axis = 1)\n",
    "\n",
    "  # Read in the content id vocabulary so we can tie the predicted class indices to their respective content ids\n",
    "  content_id_names = tf.constant(value = [x.rstrip() for x in content_id_vocab])\n",
    "\n",
    "  # Gather predicted class names based predicted class indices\n",
    "  predicted_class_names = tf.gather(params = content_id_names, indices = predicted_classes)\n",
    "\n",
    "  # If the mode is prediction\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Create predictions dict\n",
    "    predictions_dict = {\n",
    "        'class_ids': tf.expand_dims(input = predicted_classes, axis = -1),\n",
    "        'class_names' : tf.expand_dims(input = predicted_class_names, axis = -1),\n",
    "        'probabilities': tf.nn.softmax(logits = logits),\n",
    "        'logits': logits\n",
    "    }\n",
    "\n",
    "    # Create export outputs\n",
    "    export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec( # return early since we're done with what we need for prediction mode\n",
    "      mode = mode,\n",
    "      predictions = predictions_dict,\n",
    "      loss = None,\n",
    "      train_op = None,\n",
    "      eval_metric_ops = None,\n",
    "      export_outputs = export_outputs)\n",
    "\n",
    "  # Continue on with training and evaluation modes\n",
    "\n",
    "  # Create lookup table using our content id vocabulary\n",
    "  table = tf.contrib.lookup.index_table_from_file(\n",
    "    vocabulary_file = tf.gfile.Glob(filename = \"data/content_id_vocab*\")[0])\n",
    "\n",
    "  # Look up labels from vocabulary table\n",
    "  labels = table.lookup(keys = labels)\n",
    "\n",
    "  # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits = logits)\n",
    "\n",
    "  # Compute evaluation metrics of total accuracy and the accuracy of the top k classes\n",
    "  accuracy = tf.metrics.accuracy(labels = labels, predictions = predicted_classes, name = 'acc_op')\n",
    "  top_k_accuracy = tf.metrics.mean(values = tf.nn.in_top_k(predictions = logits, targets = labels, k = params['top_k']))\n",
    "  map_at_k = tf.metrics.average_precision_at_k(labels = labels, predictions = predicted_classes, k = params['top_k'])\n",
    "\n",
    "  # Put eval metrics into a dictionary\n",
    "  eval_metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'top_k_accuracy': top_k_accuracy,\n",
    "    'map_at_k': map_at_k}\n",
    "\n",
    "  # Create scalar summaries to see in TensorBoard\n",
    "  tf.summary.scalar(name = 'accuracy', tensor = accuracy[1])\n",
    "  tf.summary.scalar(name = 'top_k_accuracy', tensor = top_k_accuracy[1])\n",
    "  tf.summary.scalar(name = 'map_at_k', tensor = map_at_k[1])\n",
    "\n",
    "  # If the mode is evaluation\n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    return tf.estimator.EstimatorSpec( # return early since we're done with what we need for evaluation mode\n",
    "        mode = mode,\n",
    "        predictions = None,\n",
    "        loss = loss,\n",
    "        train_op = None,\n",
    "        eval_metric_ops = eval_metrics,\n",
    "        export_outputs = None)\n",
    "\n",
    "  # Continue on with training mode\n",
    "\n",
    "  # If the mode is training\n",
    "  assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "  # Create a custom optimizer\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate = params['learning_rate'])\n",
    "\n",
    "  # Create train op\n",
    "  train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
    "\n",
    "  return tf.estimator.EstimatorSpec( # final return since we're done with what we need for training mode\n",
    "    mode = mode,\n",
    "    predictions = None,\n",
    "    loss = loss,\n",
    "    train_op = train_op,\n",
    "    eval_metric_ops = None,\n",
    "    export_outputs = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a serving input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function\n",
    "def serving_input_fn():  \n",
    "  feature_placeholders = {\n",
    "    colname : tf.placeholder(dtype = tf.string, shape = [None]) \\\n",
    "    for colname in 'user_id,content_id,title,section_1,tag_1,user_factors,items_factors,doc2vec_avg,gender'.split(',')\n",
    "  }\n",
    "  feature_placeholders['age'] = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "  \n",
    "  features = {\n",
    "    key: tf.expand_dims(tensor, -1) \\\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "  }\n",
    "    \n",
    "  return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of the pieces are assembled let's create and run our train and evaluate loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and evaluate loop to combine all of the pieces together.\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "def train_and_evaluate(args):\n",
    "  estimator = tf.estimator.Estimator(\n",
    "    model_fn = model_fn,\n",
    "    model_dir = args['output_dir'],\n",
    "    params={\n",
    "      #'feature_columns': create_feature_columns(args),\n",
    "      'hidden_units': args['hidden_units'],\n",
    "      'n_classes': number_of_content_ids,\n",
    "      'learning_rate': args['learning_rate'],\n",
    "      'top_k': args['top_k'],\n",
    "      'bucket': args['bucket']\n",
    "    })\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = read_dataset(filename = args['train_data_paths'], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args['batch_size']),\n",
    "    max_steps = args['train_steps'])\n",
    "\n",
    "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = read_dataset(filename = args['eval_data_paths'], mode = tf.estimator.ModeKeys.EVAL, batch_size = args['batch_size']),\n",
    "    steps = None,\n",
    "    start_delay_secs = args['start_delay_secs'],\n",
    "    throttle_secs = args['throttle_secs'],\n",
    "    exporters = exporter)\n",
    "\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run train_and_evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 18:37:12.971140 139879471625984 tf_logging.py:116] Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_summary_steps': 100, '_session_config': None, '_task_id': 0, '_save_checkpoints_secs': 600, '_evaluation_master': '', '_tf_random_seed': None, '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_master': '', '_task_type': 'worker', '_service': None, '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_global_id_in_cluster': 0, '_train_distribute': None, '_model_dir': 'hybrid_recommendation_trained', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37a4381b70>, '_save_checkpoints_steps': None, '_is_chief': True, '_num_ps_replicas': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 18:37:12.973239 139879471625984 tf_logging.py:116] Using config: {'_save_summary_steps': 100, '_session_config': None, '_task_id': 0, '_save_checkpoints_secs': 600, '_evaluation_master': '', '_tf_random_seed': None, '_num_worker_replicas': 1, '_keep_checkpoint_max': 5, '_master': '', '_task_type': 'worker', '_service': None, '_log_step_count_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_global_id_in_cluster': 0, '_train_distribute': None, '_model_dir': 'hybrid_recommendation_trained', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37a4381b70>, '_save_checkpoints_steps': None, '_is_chief': True, '_num_ps_replicas': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 18:37:12.976098 139879471625984 tf_logging.py:116] Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 30 secs (eval_spec.throttle_secs) or training is finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 18:37:12.977734 139879471625984 tf_logging.py:116] Start train and evaluate loop. The evaluate will happen after 30 secs (eval_spec.throttle_secs) or training is finished.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of a default must be a length-0 or length-1 vector for 'DecodeCSV' (op: 'DecodeCSV') with input shapes: [], [13].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape of a default must be a length-0 or length-1 vector for 'DecodeCSV' (op: 'DecodeCSV') with input shapes: [], [13].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-7ad042fa3011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-162-a8e54af0f0fb>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     27\u001b[0m     exporters = exporter)\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    437\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m   \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    517\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m           hooks=train_hooks)\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m       \u001b[0;31m# Final export signal: For any eval result with global_step >= train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    841\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    851\u001b[0m       features, labels, input_hooks = (\n\u001b[1;32m    852\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[0;32m--> 853\u001b[0;31m               input_fn, model_fn_lib.ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m    854\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m       estimator_spec = self._call_model_fn(\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m    689\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_features_and_labels_from_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m     \u001b[0;31m# TODO(anjalisridhar): What about the default DistributionStrategy? Perhaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;31m# using any input is alright in that case. There is also a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-54907050e9fd>\u001b[0m in \u001b[0;36m_input_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Create dataset from file list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextLineDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \"\"\"\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mParallelMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_map_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    482\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;31m# Adds this function into 'g'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m       \u001b[0;31m# Call func and gather the output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m       \u001b[0;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mtf_map_func\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1802\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m       \u001b[0;31m# If `map_func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-54907050e9fd>\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(value_column)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOLUMNS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABEL_COLUMN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, name, na_value)\u001b[0m\n\u001b[1;32m   1213\u001b[0m       \u001b[0muse_quote_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_quote_delim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m       \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, name)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m\"DecodeCSV\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord_defaults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mfield_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfield_delim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_quote_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_quote_delim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         na_value=na_value, name=name)\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, data_types, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     return super(_FuncGraph, self).create_op(op_type, inputs, data_types,\n\u001b[0;32m--> 686\u001b[0;31m                                              **kwargs)\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of a default must be a length-0 or length-1 vector for 'DecodeCSV' (op: 'DecodeCSV') with input shapes: [], [13]."
     ]
    }
   ],
   "source": [
    "# Call train and evaluate loop\n",
    "import shutil\n",
    "\n",
    "outdir = 'hybrid_recommendation_trained'\n",
    "shutil.rmtree(outdir, ignore_errors = True) # start fresh each time\n",
    "\n",
    "arguments = {\n",
    "  'bucket': BUCKET,\n",
    "  'train_data_paths': \"gs://{}/hybrid/hybrid_dataset_train.csv*\".format(BUCKET),\n",
    "  'eval_data_paths': \"gs://{}/hybrid_/hybrid_dataset_eval.csv*\".format(BUCKET),\n",
    "  'output_dir': outdir,\n",
    "  'batch_size': 128,\n",
    "  'learning_rate': 0.1,\n",
    "  'hidden_units': [256, 128, 64],\n",
    "  'content_id_embedding_dimensions': 10,\n",
    "  'category_embedding_dimensions': 10,\n",
    "  'tag_embedding_dimensions':10,\n",
    "  'top_k': 10,\n",
    "  'train_steps': 1000,\n",
    "  'start_delay_secs': 30,\n",
    "  'throttle_secs': 30\n",
    "}\n",
    "\n",
    "train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on module locally\n",
    "\n",
    "Now let's place our code into a python module with model.py and task.py files so that we can train using Google Cloud's ML Engine! First, let's test our module locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile requirements.txt\n",
    "tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf hybrid_recommendation_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/hybrid_recommendations_module\n",
    "python -m trainer.task \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on Google Cloud ML Engine\n",
    "If our module locally trained fine, let's now use of the power of ML Engine to scale it out on Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/small_trained_model\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 1\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.01\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: ['1024 512 256', '1024 512 128', '1024 256 128', '512 256 128', '1024 512 64', '1024 256 64', '512 256 64', '1024 128 64', '512 128 64', '256 128 64', '1024 512 32', '1024 256 32', '512 256 32', '1024 128 32', '512 128 32', '256 128 32', '1024 64 32', '512 64 32', '256 64 32', '128 64 32']\n",
    "    - parameterName: content_id_embedding_dimensions\n",
    "      type: INTEGER\n",
    "      minValue: 5\n",
    "      maxValue: 250\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: author_embedding_dimensions\n",
    "      type: INTEGER\n",
    "      minValue: 5\n",
    "      maxValue: 30\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/hypertuning\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the best hyperparameters, run a big training job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/big_trained_model\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=10000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
