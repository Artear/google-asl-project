{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network hybrid recommendation system on Google Analytics data model and training\n",
    "\n",
    "This notebook demonstrates how to implement a hybrid recommendation system using a neural network to combine content-based and collaborative filtering recommendation models using Google Analytics data. We are going to use the learned user embeddings from [wals.ipynb](../wals.ipynb) and combine that with our previous content-based features from [content_based_using_neural_networks.ipynb](../content_based_using_neural_networks.ipynb)\n",
    "\n",
    "Now that we have our data preprocessed from BigQuery and Cloud Dataflow, we can build our neural network hybrid recommendation model to our preprocessed data. Then we can train locally to make sure everything works and then use the power of Google Cloud ML Engine to scale it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use TensorFlow Hub to use trained text embeddings, so let's first pip install that and reset our session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Using cached https://files.pythonhosted.org/packages/9e/f0/3a3ced04c8359e562f1b91918d9bde797c8a916fcfeddc8dc5d673d1be20/tensorflow_hub-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (1.14.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from tensorflow_hub) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/py3env/lib/python3.5/site-packages (from protobuf>=3.4.0->tensorflow_hub) (40.2.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reset the notebook's session kernel! Since we're no longer using Cloud Dataflow, we'll be using the python3 kernel from here on out so don't forget to change the kernel if it's still python2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0308 08:55:24.931139 139746844423936 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "# Import helpful libraries and setup our project, bucket, and region\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "PROJECT = 'qwiklabs-gcp-4a684069c4776675'\n",
    "BUCKET = 'colaborative-filtering-agea'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "NUMBER_OF_DAYS=5\n",
    "# do not change these\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_query(number_of_days, eval):\n",
    "  query = \"\"\"\n",
    "    SELECT \n",
    "     h.*\n",
    "    FROM `AGEA_ASL.Dataset_Hybrid` h\n",
    "    JOIN `AGEA_ASL.Dataset_A`a ON h.content_id = a.content_id\n",
    "    WHERE date_diff(CAST('2018-12-31' as DATE),CAST(a.day_write as DATE), day) < {}\n",
    "  \"\"\".format(number_of_days)\n",
    "  \n",
    "  if(eval):\n",
    "    query+= \"AND ABS(MOD(hashid),10) = 9\"\n",
    "  else:\n",
    "    query+= \"AND ABS(MOD(hashid),10) < 9\"\n",
    "  return query\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hybrid_data(number_of_days, eval):\n",
    "  query = create_hybrid_query(number_of_days,eval)\n",
    "  return bq.Query(query).execute().result().to_dataframe()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RequestException",
     "evalue": "HTTP request failed: Error encountered during execution. Retrying may solve the problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRequestException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-088a4d9e8e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhybrid_eval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hybrid_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUMBER_OF_DAYS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-22064991368a>\u001b[0m in \u001b[0;36mget_hybrid_data\u001b[0;34m(number_of_days, eval)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_hybrid_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_days\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_hybrid_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_days\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/google/datalab/bigquery/_query.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, output_options, sampling, context, query_params)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \"\"\"\n\u001b[1;32m    338\u001b[0m     return self.execute_async(output_options, sampling=sampling, context=context,\n\u001b[0;32m--> 339\u001b[0;31m                               query_params=query_params).wait()\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/google/datalab/bigquery/_query.py\u001b[0m in \u001b[0;36mexecute_async\u001b[0;34m(self, output_options, sampling, context, query_params)\u001b[0m\n\u001b[1;32m    270\u001b[0m                                            query_params=query_params)\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'jobReference'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unexpected response from server'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/google/datalab/bigquery/_query.py\u001b[0m in \u001b[0;36mexecute_async\u001b[0;34m(self, output_options, sampling, context, query_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m                                            \u001b[0mallow_large_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_large_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                                            \u001b[0mtable_definitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                                            query_params=query_params)\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/google/datalab/bigquery/_api.py\u001b[0m in \u001b[0;36mjobs_insert_query\u001b[0;34m(self, sql, table_name, append, overwrite, dry_run, use_cache, batch, allow_large_results, table_definitions, query_params)\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0mquery_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'queryParameters'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatalab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mjobs_query_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/google/datalab/utils/_http.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(url, args, data, headers, method, credentials, raw_response, stats)\u001b[0m\n\u001b[1;32m    156\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRequestException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to process HTTP response.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRequestException\u001b[0m: HTTP request failed: Error encountered during execution. Retrying may solve the problem."
     ]
    }
   ],
   "source": [
    "hybrid_eval_data = get_hybrid_data(NUMBER_OF_DAYS, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT \n",
      "     h.*\n",
      "    FROM `AGEA_ASL.Dataset_Hybrid` h\n",
      "    JOIN `AGEA_ASL.Dataset_A`a ON h.content_id = a.content_id\n",
      "    WHERE date_diff(CAST('2018-12-31' as DATE),CAST(a.day_write as DATE), day) < 5\n",
      "  AND ABS(MOD(hashid),10) = 9\n"
     ]
    }
   ],
   "source": [
    "print(create_hybrid_query(5,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>title</th>\n",
       "      <th>section_1</th>\n",
       "      <th>tag_1</th>\n",
       "      <th>d2v</th>\n",
       "      <th>user_factors</th>\n",
       "      <th>item_factors</th>\n",
       "      <th>hash_id</th>\n",
       "      <th>next_article</th>\n",
       "      <th>doc2vec_avg</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6514786</td>\n",
       "      <td>2s7OTElpi</td>\n",
       "      <td>Bienes Personales: últimos días para ordenar l...</td>\n",
       "      <td>Economía - Economía</td>\n",
       "      <td>Bienes Personales</td>\n",
       "      <td>0.33847874402999878|1.3664971590042114|-0.7889...</td>\n",
       "      <td>-0.00033484702|-6.0474045e-05|-6.0434253e-05|-...</td>\n",
       "      <td>0.0012098223|-0.00017613063|0.0009774928|0.000...</td>\n",
       "      <td>2664641251200580223</td>\n",
       "      <td>IOkphCt3R</td>\n",
       "      <td>0.18227090779691935|0.92645788192749023|0.0595...</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4199496</td>\n",
       "      <td>ta_fYYVEJ</td>\n",
       "      <td>Se quedaron sin trabajo, empezaron a hacer via...</td>\n",
       "      <td>Zonales</td>\n",
       "      <td>emprendedor</td>\n",
       "      <td>1.5587911605834961|0.59180998802185059|0.37883...</td>\n",
       "      <td>0.0012058287|0.0034401685|0.002415978|-0.00051...</td>\n",
       "      <td>-0.003168901|-0.0015254492|0.0012588227|-0.001...</td>\n",
       "      <td>7672729398365402795</td>\n",
       "      <td>oiTew0bSK</td>\n",
       "      <td>0.66715850432713852|0.16961873241368383|0.2643...</td>\n",
       "      <td>Femenino</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4090930</td>\n",
       "      <td>15B10yMcS</td>\n",
       "      <td>Ante el cambio de reglamento, Serena Williams ...</td>\n",
       "      <td>Deportes - Tenis</td>\n",
       "      <td>Serena Williams</td>\n",
       "      <td>1.7834646701812742|0.30468490719795227|-0.6643...</td>\n",
       "      <td>-0.00023394148|0.000811376|-0.00035726337|-0.0...</td>\n",
       "      <td>0.0005940497|0.00024220906|0.00028581568|-0.00...</td>\n",
       "      <td>667019861704056012</td>\n",
       "      <td>wcSa3B96M</td>\n",
       "      <td>0.760086470271779|0.48489618667436746|0.122618...</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3338903</td>\n",
       "      <td>H96jQUNxP</td>\n",
       "      <td>Detuvieron al ex presidente de Lotería en el g...</td>\n",
       "      <td>Política</td>\n",
       "      <td>Daniel Scioli</td>\n",
       "      <td>0.38895833492279047|1.6486941576004031|0.12129...</td>\n",
       "      <td>0.00039643896|0.0034561933|0.00031241548|-0.00...</td>\n",
       "      <td>0.00091878977|0.0014115523|0.00014006016|-0.00...</td>\n",
       "      <td>-3357196873604198869</td>\n",
       "      <td>O2penpb5A</td>\n",
       "      <td>0.48551574687037319|0.24760270877785473|0.1369...</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4424738</td>\n",
       "      <td>uU9fSN3rZ</td>\n",
       "      <td>Prioridad para Mauricio Macri: evitar cualquie...</td>\n",
       "      <td>Opinión</td>\n",
       "      <td>Política económica</td>\n",
       "      <td>-0.5431375503540038|1.5081772804260252|0.28818...</td>\n",
       "      <td>-0.000107121065|0.0017253754|0.004925742|-0.00...</td>\n",
       "      <td>0.0005537326|-0.0015965832|0.00082380266|-0.00...</td>\n",
       "      <td>-5588144826490365062</td>\n",
       "      <td>F-MfH0l1C</td>\n",
       "      <td>0.71932929782615818|0.6298682553384285|0.21361...</td>\n",
       "      <td>Masculino</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id content_id                                              title  \\\n",
       "0  6514786  2s7OTElpi  Bienes Personales: últimos días para ordenar l...   \n",
       "1  4199496  ta_fYYVEJ  Se quedaron sin trabajo, empezaron a hacer via...   \n",
       "2  4090930  15B10yMcS  Ante el cambio de reglamento, Serena Williams ...   \n",
       "3  3338903  H96jQUNxP  Detuvieron al ex presidente de Lotería en el g...   \n",
       "4  4424738  uU9fSN3rZ  Prioridad para Mauricio Macri: evitar cualquie...   \n",
       "\n",
       "             section_1               tag_1  \\\n",
       "0  Economía - Economía   Bienes Personales   \n",
       "1              Zonales         emprendedor   \n",
       "2     Deportes - Tenis     Serena Williams   \n",
       "3             Política       Daniel Scioli   \n",
       "4              Opinión  Política económica   \n",
       "\n",
       "                                                 d2v  \\\n",
       "0  0.33847874402999878|1.3664971590042114|-0.7889...   \n",
       "1  1.5587911605834961|0.59180998802185059|0.37883...   \n",
       "2  1.7834646701812742|0.30468490719795227|-0.6643...   \n",
       "3  0.38895833492279047|1.6486941576004031|0.12129...   \n",
       "4  -0.5431375503540038|1.5081772804260252|0.28818...   \n",
       "\n",
       "                                        user_factors  \\\n",
       "0  -0.00033484702|-6.0474045e-05|-6.0434253e-05|-...   \n",
       "1  0.0012058287|0.0034401685|0.002415978|-0.00051...   \n",
       "2  -0.00023394148|0.000811376|-0.00035726337|-0.0...   \n",
       "3  0.00039643896|0.0034561933|0.00031241548|-0.00...   \n",
       "4  -0.000107121065|0.0017253754|0.004925742|-0.00...   \n",
       "\n",
       "                                        item_factors              hash_id  \\\n",
       "0  0.0012098223|-0.00017613063|0.0009774928|0.000...  2664641251200580223   \n",
       "1  -0.003168901|-0.0015254492|0.0012588227|-0.001...  7672729398365402795   \n",
       "2  0.0005940497|0.00024220906|0.00028581568|-0.00...   667019861704056012   \n",
       "3  0.00091878977|0.0014115523|0.00014006016|-0.00... -3357196873604198869   \n",
       "4  0.0005537326|-0.0015965832|0.00082380266|-0.00... -5588144826490365062   \n",
       "\n",
       "  next_article                                        doc2vec_avg     gender  \\\n",
       "0    IOkphCt3R  0.18227090779691935|0.92645788192749023|0.0595...  Masculino   \n",
       "1    oiTew0bSK  0.66715850432713852|0.16961873241368383|0.2643...   Femenino   \n",
       "2    wcSa3B96M  0.760086470271779|0.48489618667436746|0.122618...  Masculino   \n",
       "3    O2penpb5A  0.48551574687037319|0.24760270877785473|0.1369...  Masculino   \n",
       "4    F-MfH0l1C  0.71932929782615818|0.6298682553384285|0.21361...  Masculino   \n",
       "\n",
       "   age  \n",
       "0   52  \n",
       "1   33  \n",
       "2   65  \n",
       "3   65  \n",
       "4   45  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_eval_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_data.to_csv('data/hybrid_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/hybrid_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/hybrid_dataset.csv [Content-Type=text/csv]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1 files][  3.4 GiB/  3.4 GiB]   64.9 MiB/s                                   \n",
      "Operation completed over 1 objects/3.4 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/hybrid_dataset.csv gs://{BUCKET}/hybrid/hybrid_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_distinct_values(column_name):\n",
    "  query = \"\"\"\n",
    "    SELECT COUNT(DISTINCT {}) FROM \n",
    "      ({})\n",
    "  \"\"\".format(column_name, create_hybrid_query(NUMBER_OF_DAYS))\n",
    "  return bq.Query(query).execute().result().to_dataframe()['f0_'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_content_ids = count_distinct_values('content_id')\n",
    "number_of_sections = count_distinct_values('section_1')\n",
    "number_of_tags = count_distinct_values('tag_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_values(column_name):\n",
    "  query = \"\"\"\n",
    "    SELECT DISTINCT {} FROM \n",
    "      ({})\n",
    "  \"\"\".format(column_name, create_hybrid_query(NUMBER_OF_DAYS))\n",
    "  return bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_vocab = get_column_values('section_1')['section_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_id_vocab = get_column_values('content_id')['content_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_id_vocab.to_csv('data/content_id_vocab',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       SZS09KtDg\n",
       "1       X4CfOaDyg\n",
       "2       HGTJStU3s\n",
       "3       64dpFjJN_\n",
       "4       S2sNW6lDY\n",
       "5       _WwJSZdbk\n",
       "6       vZfb16nCk\n",
       "7       Ja4PWSjaz\n",
       "8       5ppYN8POW\n",
       "9       9rfMRDoFO\n",
       "10      rldlKBq2C\n",
       "11      raDGLLyrZ\n",
       "12      mSSLe2GBm\n",
       "13      _9DeC9xn1\n",
       "14      FeMgVJ5Qo\n",
       "15      TzpzZ-uMx\n",
       "16      mETwtqVGB\n",
       "17      hHdzPY2jQ\n",
       "18      M9Wv9bDdA\n",
       "19      ff--Hncaw\n",
       "20      MN9bvZAp2\n",
       "21      QVHubOLdV\n",
       "22      ptrIsb4Dm\n",
       "23      nainTnpx7\n",
       "24      c_7IeBJqw\n",
       "25      Sy3rTnW5A\n",
       "26      1sXplEm64\n",
       "27      9I5rLIG2E\n",
       "28      MFsQoTsD2\n",
       "29      Lb5HXpNnC\n",
       "          ...    \n",
       "718     Grf0nmw68\n",
       "719     HSDfLxUaF\n",
       "720     9Xrr5gjiJ\n",
       "721     kOtaMi_l2\n",
       "722     lDx6T4BHp\n",
       "723     QxW9TppnN\n",
       "724     OH-rKdx2l\n",
       "725     vjPu4Bbly\n",
       "726     rFcjETD8L\n",
       "727     AsIsPJf6R\n",
       "728     m2RgxwypH\n",
       "729     m0abkgXkd\n",
       "730     VgZhdRpJs\n",
       "731     -JqVaE1D5\n",
       "732     -V8U49SYW\n",
       "733     872l_sWx5\n",
       "734     tQLeGoH7z\n",
       "735     ij91Y3pqU\n",
       "736     Kl6Thgu5h\n",
       "737     WHRaRCA3M\n",
       "738     wUKCfmS2_\n",
       "739     kXiog-HZx\n",
       "740     ToRnbQubP\n",
       "741     Cq0RWDE7M\n",
       "742     Pzuuheomu\n",
       "743     JZAG5Ysgr\n",
       "744     _T3k7v1dG\n",
       "745     Ou7phZlH6\n",
       "746     hta4AaOj2\n",
       "747    Hr7-ZUTa3M\n",
       "Name: content_id, Length: 748, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_id_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create hybrid recommendation system model using TensorFlow </h2>\n",
    "\n",
    "Now that we've created our training and evaluation input files as well as our categorical feature vocabulary files, we can create our TensorFlow hybrid recommendation system model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get some of our aggregate information that we will use in the model from some of our preprocessed files we saved in Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.lib.io import file_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine CSV and label columns\n",
    "COLUMNS = 'user_id,content_id,title,section_1,tag_1,d2v,user_factors,items_factors,doc2vec_avg,gender,age'.split(',')\n",
    "LABEL_COLUMN = 'next_content_id'\n",
    "\n",
    "#FACTOR_COLUMNS = ['d2v_'+i for i in range(50)] + ['user_factor_'+i for i in range(10)] + ['item_factor_'+i for i in range(10)] + ['d2v_avg'+i for i in range(50)]\n",
    "# Set default values for each CSV column\n",
    "DEFAULTS = [\"Unknown\"*len(COLUMNS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input function for training and evaluation to read from our preprocessed CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input function for train and eval\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "  def _input_fn():\n",
    "    def decode_csv(value_column):\n",
    "      columns = tf.decode_csv(records = value_column, record_defaults = DEFAULTS)\n",
    "      features = dict(zip(COLUMNS, columns))          \n",
    "      label = features.pop(LABEL_COLUMN)         \n",
    "      return features, label\n",
    "\n",
    "    # Create list of files that match pattern\n",
    "    file_list = tf.gfile.Glob(filename = filename)\n",
    "\n",
    "    # Create dataset from file list\n",
    "    dataset = tf.data.TextLineDataset(filenames = file_list).map(map_func = decode_csv)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      num_epochs = None # indefinitely\n",
    "      dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "    else:\n",
    "      num_epochs = 1 # end-of-input after this\n",
    "\n",
    "    dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create our feature columns using our read in features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature columns to be used in model\n",
    "def create_feature_columns(args):\n",
    "  # Create content_id feature column\n",
    "  content_id_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key = \"content_id\",\n",
    "    hash_bucket_size = number_of_content_ids)\n",
    "\n",
    "  # Embed content id into a lower dimensional representation\n",
    "  embedded_content_column = tf.feature_column.embedding_column(\n",
    "    categorical_column = content_id_column,\n",
    "    dimension = args['content_id_embedding_dimensions'])\n",
    "\n",
    "  # Create section feature column\n",
    "  categorical_section_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    key = \"section_1\",\n",
    "    vocabulary_list=sections_vocab,\n",
    "    num_oov_buckets = 1)\n",
    "\n",
    "  # Convert section category column into indicator column so that it can be used in a DNN\n",
    "  indicator_section_column = tf.feature_column.indicator_column(categorical_column = categorical_section_column)\n",
    "\n",
    "  # Create title feature column using TF Hub\n",
    "  embedded_title_column = hub.text_embedding_column(\n",
    "    key = \"title\", \n",
    "    module_spec = \"https://tfhub.dev/google/nnlm-es-dim50-with-normalization/1\",\n",
    "    trainable = False)\n",
    "\n",
    "  # Create tag feature column\n",
    "  tag_column = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    key = \"tag_1\",\n",
    "    hash_bucket_size = number_of_tags + 1)\n",
    "\n",
    "  # Embed tag into a lower dimensional representation\n",
    "  embedded_tag_column = tf.feature_column.embedding_column(\n",
    "    categorical_column = tag_column,\n",
    "    dimension = args['tag_embedding_dimensions'])\n",
    "\n",
    "  # Create months since epoch boundaries list for our binning\n",
    "  age_boundaries = list(range(15, 100))\n",
    "\n",
    "  # Create age feature column using raw data\n",
    "  age_column = tf.feature_column.numeric_column(\n",
    "    key = \"age\")\n",
    "\n",
    "  # Create bucketized age feature column using our boundaries\n",
    "  age_bucketized = tf.feature_column.bucketized_column(\n",
    "    source_column = age_column,\n",
    "    boundaries = age_boundaries)\n",
    "\n",
    "  # Cross our categorical section column and bucketized age column\n",
    "  crossed_age_since_section_column = tf.feature_column.crossed_column(\n",
    "    keys = [categorical_section_column, age_bucketized],\n",
    "    hash_bucket_size = len(age_boundaries) * (number_of_sections + 1))\n",
    "\n",
    "  # Convert crossed categorical category and bucketized months since epoch column into indicator column so that it can be used in a DNN\n",
    "  indicator_crossed_age_since_section_column = tf.feature_column.indicator_column(categorical_column = crossed_age_since_section_column)\n",
    "\n",
    "  # Create user and item factor feature columns from our trained WALS model\n",
    "  #user_factors = [tf.feature_column.numeric_column(key = \"user_factor_\" + str(i)) for i in range(10)]\n",
    "  #item_factors =  [tf.feature_column.numeric_column(key = \"item_factor_\" + str(i)) for i in range(10)]\n",
    "  #d2v = [tf.feature_column.numeric_column(key = \"d2v_\" + str(i)) for i in range(50)]\n",
    "  #d2v_avg = [tf.feature_column.numeric_column(key = \"d2v_avg_\" + str(i)) for i in range(50)]\n",
    "  \n",
    "  # Create list of feature columns\n",
    "  feature_columns = [embedded_content_column,\n",
    "                     embedded_tag_column,\n",
    "                     indicator_section_column,\n",
    "                     embedded_title_column,\n",
    "                     crossed_age_since_section_column] #+ user_factors + item_factors + d2v + d2v_avg\n",
    "\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom model function for our custom estimator\n",
    "def model_fn(features, labels, mode, params):\n",
    "  # Create neural network input layer using our feature columns defined above\n",
    "  net = tf.feature_column.input_layer(features = features, feature_columns = params['feature_columns'])\n",
    "\n",
    "  # Create hidden layers by looping through hidden unit list\n",
    "  for units in params['hidden_units']:\n",
    "    net = tf.layers.dense(inputs = net, units = units, activation = tf.nn.relu)\n",
    "\n",
    "  # Compute logits (1 per class) using the output of our last hidden layer\n",
    "  logits = tf.layers.dense(inputs = net, units = params['n_classes'], activation = None)\n",
    "\n",
    "  # Find the predicted class indices based on the highest logit (which will result in the highest probability)\n",
    "  predicted_classes = tf.argmax(input = logits, axis = 1)\n",
    "\n",
    "  # Read in the content id vocabulary so we can tie the predicted class indices to their respective content ids\n",
    "  content_id_names = tf.constant(value = [x.rstrip() for x in content_id_vocab])\n",
    "\n",
    "  # Gather predicted class names based predicted class indices\n",
    "  predicted_class_names = tf.gather(params = content_id_names, indices = predicted_classes)\n",
    "\n",
    "  # If the mode is prediction\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Create predictions dict\n",
    "    predictions_dict = {\n",
    "        'class_ids': tf.expand_dims(input = predicted_classes, axis = -1),\n",
    "        'class_names' : tf.expand_dims(input = predicted_class_names, axis = -1),\n",
    "        'probabilities': tf.nn.softmax(logits = logits),\n",
    "        'logits': logits\n",
    "    }\n",
    "\n",
    "    # Create export outputs\n",
    "    export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = predictions_dict)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec( # return early since we're done with what we need for prediction mode\n",
    "      mode = mode,\n",
    "      predictions = predictions_dict,\n",
    "      loss = None,\n",
    "      train_op = None,\n",
    "      eval_metric_ops = None,\n",
    "      export_outputs = export_outputs)\n",
    "\n",
    "  # Continue on with training and evaluation modes\n",
    "\n",
    "  # Create lookup table using our content id vocabulary\n",
    "  table = tf.contrib.lookup.index_table_from_file(\n",
    "    vocabulary_file = tf.gfile.Glob(filename = \"data/content_id_vocab*\")[0])\n",
    "\n",
    "  # Look up labels from vocabulary table\n",
    "  labels = table.lookup(keys = labels)\n",
    "\n",
    "  # Compute loss using sparse softmax cross entropy since this is classification and our labels (content id indices) and probabilities are mutually exclusive\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy(labels = labels, logits = logits)\n",
    "\n",
    "  # Compute evaluation metrics of total accuracy and the accuracy of the top k classes\n",
    "  accuracy = tf.metrics.accuracy(labels = labels, predictions = predicted_classes, name = 'acc_op')\n",
    "  top_k_accuracy = tf.metrics.mean(values = tf.nn.in_top_k(predictions = logits, targets = labels, k = params['top_k']))\n",
    "  map_at_k = tf.metrics.average_precision_at_k(labels = labels, predictions = predicted_classes, k = params['top_k'])\n",
    "\n",
    "  # Put eval metrics into a dictionary\n",
    "  eval_metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    'top_k_accuracy': top_k_accuracy,\n",
    "    'map_at_k': map_at_k}\n",
    "\n",
    "  # Create scalar summaries to see in TensorBoard\n",
    "  tf.summary.scalar(name = 'accuracy', tensor = accuracy[1])\n",
    "  tf.summary.scalar(name = 'top_k_accuracy', tensor = top_k_accuracy[1])\n",
    "  tf.summary.scalar(name = 'map_at_k', tensor = map_at_k[1])\n",
    "\n",
    "  # If the mode is evaluation\n",
    "  if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    return tf.estimator.EstimatorSpec( # return early since we're done with what we need for evaluation mode\n",
    "        mode = mode,\n",
    "        predictions = None,\n",
    "        loss = loss,\n",
    "        train_op = None,\n",
    "        eval_metric_ops = eval_metrics,\n",
    "        export_outputs = None)\n",
    "\n",
    "  # Continue on with training mode\n",
    "\n",
    "  # If the mode is training\n",
    "  assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "  # Create a custom optimizer\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate = params['learning_rate'])\n",
    "\n",
    "  # Create train op\n",
    "  train_op = optimizer.minimize(loss = loss, global_step = tf.train.get_global_step())\n",
    "\n",
    "  return tf.estimator.EstimatorSpec( # final return since we're done with what we need for training mode\n",
    "    mode = mode,\n",
    "    predictions = None,\n",
    "    loss = loss,\n",
    "    train_op = train_op,\n",
    "    eval_metric_ops = None,\n",
    "    export_outputs = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a serving input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function\n",
    "def serving_input_fn():  \n",
    "  feature_placeholders = {\n",
    "    colname : tf.placeholder(dtype = tf.string, shape = [None]) \\\n",
    "    for colname in 'user_id,content_id,title,section_1,tag_1,user_factors,items_factors,doc2vec_avg,gender'.split(',')\n",
    "  }\n",
    "  feature_placeholders['age'] = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "  \n",
    "  features = {\n",
    "    key: tf.expand_dims(tensor, -1) \\\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "  }\n",
    "    \n",
    "  return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of the pieces are assembled let's create and run our train and evaluate loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and evaluate loop to combine all of the pieces together.\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "def train_and_evaluate(args):\n",
    "  estimator = tf.estimator.Estimator(\n",
    "    model_fn = model_fn,\n",
    "    model_dir = args['output_dir'],\n",
    "    params={\n",
    "      'feature_columns': create_feature_columns(args),\n",
    "      'hidden_units': args['hidden_units'],\n",
    "      'n_classes': number_of_content_ids,\n",
    "      'learning_rate': args['learning_rate'],\n",
    "      'top_k': args['top_k'],\n",
    "      'bucket': args['bucket']\n",
    "    })\n",
    "\n",
    "  train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = read_dataset(filename = args['train_data_paths'], mode = tf.estimator.ModeKeys.TRAIN, batch_size = args['batch_size']),\n",
    "    max_steps = args['train_steps'])\n",
    "\n",
    "  exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "  eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = read_dataset(filename = args['eval_data_paths'], mode = tf.estimator.ModeKeys.EVAL, batch_size = args['batch_size']),\n",
    "    steps = None,\n",
    "    start_delay_secs = args['start_delay_secs'],\n",
    "    throttle_secs = args['throttle_secs'],\n",
    "    exporters = exporter)\n",
    "\n",
    "  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run train_and_evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 08:44:30.473973 140367442020096 tf_logging.py:116] Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa938da3080>, '_service': None, '_save_checkpoints_steps': None, '_master': '', '_keep_checkpoint_max': 5, '_session_config': None, '_task_type': 'worker', '_task_id': 0, '_log_step_count_steps': 100, '_train_distribute': None, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_global_id_in_cluster': 0, '_evaluation_master': '', '_num_worker_replicas': 1, '_model_dir': 'hybrid_recommendation_trained', '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 08:44:30.476354 140367442020096 tf_logging.py:116] Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa938da3080>, '_service': None, '_save_checkpoints_steps': None, '_master': '', '_keep_checkpoint_max': 5, '_session_config': None, '_task_type': 'worker', '_task_id': 0, '_log_step_count_steps': 100, '_train_distribute': None, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_global_id_in_cluster': 0, '_evaluation_master': '', '_num_worker_replicas': 1, '_model_dir': 'hybrid_recommendation_trained', '_is_chief': True, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 08:44:30.481549 140367442020096 tf_logging.py:116] Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 30 secs (eval_spec.throttle_secs) or training is finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 08:44:30.482992 140367442020096 tf_logging.py:116] Start train and evaluate loop. The evaluate will happen after 30 secs (eval_spec.throttle_secs) or training is finished.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 0 for 'DecodeCSV' (op: 'DecodeCSV') with input shapes: [], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 1 but is rank 0 for 'DecodeCSV' (op: 'DecodeCSV') with input shapes: [], [].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-3e3c90cab33f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-d480b6132815>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     27\u001b[0m     exporters = exporter)\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    437\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m   \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    517\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m           hooks=train_hooks)\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m       \u001b[0;31m# Final export signal: For any eval result with global_step >= train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    841\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    851\u001b[0m       features, labels, input_hooks = (\n\u001b[1;32m    852\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[0;32m--> 853\u001b[0;31m               input_fn, model_fn_lib.ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m    854\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m       estimator_spec = self._call_model_fn(\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m    689\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_features_and_labels_from_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m     \u001b[0;31m# TODO(anjalisridhar): What about the default DistributionStrategy? Perhaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;31m# using any input is alright in that case. There is also a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-7439a5d1b51d>\u001b[0m in \u001b[0;36m_input_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Create dataset from file list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextLineDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \"\"\"\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mParallelMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_map_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    482\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;31m# Adds this function into 'g'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m       \u001b[0;31m# Call func and gather the output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m       \u001b[0;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mtf_map_func\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1802\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m       \u001b[0;31m# If `map_func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-7439a5d1b51d>\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(value_column)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOLUMNS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABEL_COLUMN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, name, na_value)\u001b[0m\n\u001b[1;32m   1213\u001b[0m       \u001b[0muse_quote_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_quote_delim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m       \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mdecode_csv\u001b[0;34m(records, record_defaults, field_delim, use_quote_delim, na_value, name)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m\"DecodeCSV\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord_defaults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mfield_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfield_delim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_quote_delim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_quote_delim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         na_value=na_value, name=name)\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, data_types, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     return super(_FuncGraph, self).create_op(op_type, inputs, data_types,\n\u001b[0;32m--> 686\u001b[0;31m                                              **kwargs)\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py3env/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 1 but is rank 0 for 'DecodeCSV' (op: 'DecodeCSV') with input shapes: [], []."
     ]
    }
   ],
   "source": [
    "# Call train and evaluate loop\n",
    "import shutil\n",
    "\n",
    "outdir = 'hybrid_recommendation_trained'\n",
    "shutil.rmtree(outdir, ignore_errors = True) # start fresh each time\n",
    "\n",
    "arguments = {\n",
    "  'bucket': BUCKET,\n",
    "  'train_data_paths': \"gs://{}/hybrid_recommendation/preproc/features/train.csv*\".format(BUCKET),\n",
    "  'eval_data_paths': \"gs://{}/hybrid_recommendation/preproc/features/eval.csv*\".format(BUCKET),\n",
    "  'output_dir': outdir,\n",
    "  'batch_size': 128,\n",
    "  'learning_rate': 0.1,\n",
    "  'hidden_units': [256, 128, 64],\n",
    "  'content_id_embedding_dimensions': 10,\n",
    "  'category_embedding_dimensions': 10,\n",
    "  'tag_embedding_dimensions':10,\n",
    "  'top_k': 10,\n",
    "  'train_steps': 1000,\n",
    "  'start_delay_secs': 30,\n",
    "  'throttle_secs': 30\n",
    "}\n",
    "\n",
    "train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on module locally\n",
    "\n",
    "Now let's place our code into a python module with model.py and task.py files so that we can train using Google Cloud's ML Engine! First, let's test our module locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile requirements.txt\n",
    "tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "echo \"bucket=${BUCKET}\"\n",
    "rm -rf hybrid_recommendation_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/hybrid_recommendations_module\n",
    "python -m trainer.task \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on Google Cloud ML Engine\n",
    "If our module locally trained fine, let's now use of the power of ML Engine to scale it out on Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/small_trained_model\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 1\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.01\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: ['1024 512 256', '1024 512 128', '1024 256 128', '512 256 128', '1024 512 64', '1024 256 64', '512 256 64', '1024 128 64', '512 128 64', '256 128 64', '1024 512 32', '1024 256 32', '512 256 32', '1024 128 32', '512 128 32', '256 128 32', '1024 64 32', '512 64 32', '256 64 32', '128 64 32']\n",
    "    - parameterName: content_id_embedding_dimensions\n",
    "      type: INTEGER\n",
    "      minValue: 5\n",
    "      maxValue: 250\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: author_embedding_dimensions\n",
    "      type: INTEGER\n",
    "      minValue: 5\n",
    "      maxValue: 30\n",
    "      scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/hypertuning\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  --config=hyperparam.yaml \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=1000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the best hyperparameters, run a big training job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "OUTDIR=gs://${BUCKET}/hybrid_recommendation/big_trained_model\n",
    "JOBNAME=hybrid_recommendation_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=$(pwd)/hybrid_recommendations_module/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --bucket=${BUCKET} \\\n",
    "  --train_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/train.csv* \\\n",
    "  --eval_data_paths=gs://${BUCKET}/hybrid_recommendation/preproc/features/eval.csv* \\\n",
    "  --output_dir=${OUTDIR} \\\n",
    "  --batch_size=128 \\\n",
    "  --learning_rate=0.1 \\\n",
    "  --hidden_units=\"256 128 64\" \\\n",
    "  --content_id_embedding_dimensions=10 \\\n",
    "  --author_embedding_dimensions=10 \\\n",
    "  --top_k=10 \\\n",
    "  --train_steps=10000 \\\n",
    "  --start_delay_secs=30 \\\n",
    "  --throttle_secs=30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
